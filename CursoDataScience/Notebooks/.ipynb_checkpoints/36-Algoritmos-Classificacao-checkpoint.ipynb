{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Algoritmos de classificação</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.9.13\n",
      "Pandas Version: 2.0.2\n",
      "Matplotlib Version: 3.7.1\n",
      "Sklearn Version: 1.2.2\n"
     ]
    }
   ],
   "source": [
    "# Versão da Linguagem Python\n",
    "from platform import python_version\n",
    "print('Python Version:', python_version())\n",
    "\n",
    "# Verificando as versões dos pacotes instalados\n",
    "pandasVersion = !pip show pandas\n",
    "matplotlibVersion = !pip show matplotlib\n",
    "sklearnVersion = !pip show scikit-learn\n",
    "print('Pandas', pandasVersion[1])\n",
    "print(\"Matplotlib\", matplotlibVersion[1])\n",
    "print(\"Sklearn\", sklearnVersion[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando os pacotes\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Coletando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Survived</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Idade</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>22.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>38.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>26.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>35.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>35.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Survived  SibSp  Parch     Fare  Idade  Sex_female  Sex_male  \\\n",
       "0           0         0      1      0   7.2500   22.0       False      True   \n",
       "1           1         1      1      0  71.2833   38.0        True     False   \n",
       "2           2         1      0      0   7.9250   26.0        True     False   \n",
       "3           3         1      1      0  53.1000   35.0        True     False   \n",
       "4           4         0      0      0   8.0500   35.0       False      True   \n",
       "\n",
       "   Embarked_C  Embarked_Q  Embarked_S  Pclass_1  Pclass_2  Pclass_3  \n",
       "0       False       False        True       0.0       0.0       1.0  \n",
       "1        True       False       False       1.0       0.0       0.0  \n",
       "2       False       False        True       0.0       0.0       1.0  \n",
       "3       False       False        True       1.0       0.0       0.0  \n",
       "4       False       False        True       0.0       0.0       1.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Coletando os dados \n",
    "# Atenção para o dataset que foi carregado. A variável idade não tem valores nulos nem outliers neste conjunto de dados.\n",
    "# Também já criamos variáveis dummies para as colunas sexo, porto de embarque e a classe do passageiro. \n",
    "df = pd.read_csv('Dados/Titanic/titanicDummies.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Idade</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>22.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>38.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>26.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>35.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>35.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  SibSp  Parch     Fare  Idade  Sex_female  Sex_male  Embarked_C  \\\n",
       "0         0      1      0   7.2500   22.0       False      True       False   \n",
       "1         1      1      0  71.2833   38.0        True     False        True   \n",
       "2         1      0      0   7.9250   26.0        True     False       False   \n",
       "3         1      1      0  53.1000   35.0        True     False       False   \n",
       "4         0      0      0   8.0500   35.0       False      True       False   \n",
       "\n",
       "   Embarked_Q  Embarked_S  Pclass_1  Pclass_2  Pclass_3  \n",
       "0       False        True       0.0       0.0       1.0  \n",
       "1       False       False       1.0       0.0       0.0  \n",
       "2       False        True       0.0       0.0       1.0  \n",
       "3       False        True       1.0       0.0       0.0  \n",
       "4       False        True       0.0       0.0       1.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Excluindo coluna\n",
    "df = df.drop([\"Unnamed: 0\"], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01415106, 0.31014659],\n",
       "       [0.13913574, 0.54009773],\n",
       "       [0.01546857, 0.36763438],\n",
       "       ...,\n",
       "       [0.04577135, 0.32825067],\n",
       "       [0.0585561 , 0.36763438],\n",
       "       [0.01512699, 0.45386605]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retorna os valores do DataFrame e converte em uma matriz NumPy\n",
    "dados = df[[\"Fare\", \"Idade\"]].values\n",
    "\n",
    "# Cria o objeto da classe MinMaxScaler \n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "# Realiza a normalização dimensionando as variáveis em uma escala entre 0 e 1\n",
    "dadosNormalizados = min_max_scaler.fit_transform(dados)\n",
    "dadosNormalizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fare</th>\n",
       "      <th>Idade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.014151</td>\n",
       "      <td>0.310147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.139136</td>\n",
       "      <td>0.540098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.015469</td>\n",
       "      <td>0.367634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.103644</td>\n",
       "      <td>0.496982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.015713</td>\n",
       "      <td>0.496982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Fare     Idade\n",
       "0  0.014151  0.310147\n",
       "1  0.139136  0.540098\n",
       "2  0.015469  0.367634\n",
       "3  0.103644  0.496982\n",
       "4  0.015713  0.496982"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criando um DataFrame com as colunas normalizadas\n",
    "dadosNormalizados = pd.DataFrame(dadosNormalizados, columns=[\"Fare\", \"Idade\"])\n",
    "dadosNormalizados.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluindo colunas\n",
    "df = df.drop([\"Fare\", \"Idade\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Idade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>0.310147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.139136</td>\n",
       "      <td>0.540098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.015469</td>\n",
       "      <td>0.367634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.103644</td>\n",
       "      <td>0.496982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.015713</td>\n",
       "      <td>0.496982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  SibSp  Parch  Sex_female  Sex_male  Embarked_C  Embarked_Q  \\\n",
       "0         0      1      0       False      True       False       False   \n",
       "1         1      1      0        True     False        True       False   \n",
       "2         1      0      0        True     False       False       False   \n",
       "3         1      1      0        True     False       False       False   \n",
       "4         0      0      0       False      True       False       False   \n",
       "\n",
       "   Embarked_S  Pclass_1  Pclass_2  Pclass_3      Fare     Idade  \n",
       "0        True       0.0       0.0       1.0  0.014151  0.310147  \n",
       "1       False       1.0       0.0       0.0  0.139136  0.540098  \n",
       "2        True       0.0       0.0       1.0  0.015469  0.367634  \n",
       "3        True       1.0       0.0       0.0  0.103644  0.496982  \n",
       "4        True       0.0       0.0       1.0  0.015713  0.496982  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenando DataFrames\n",
    "df = pd.concat([df, dadosNormalizados], axis=\"columns\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividindo o dataset em dados de treino e dados de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando as variáveis preditoras e a variável alvo\n",
    "numeroObservacoes = len(df)\n",
    "numeroVariaveisPreditoras = 12\n",
    "X = df[[\"SibSp\", \"Parch\", \"Fare\", \"Idade\", \"Sex_female\", \"Sex_male\", \"Embarked_C\", \"Embarked_Q\", \"Embarked_S\", \"Pclass_1\", \"Pclass_2\", \"Pclass_3\"]].values.reshape((numeroObservacoes, numeroVariaveisPreditoras)) # X deve sempre ser uma matriz e nunca um vetor\n",
    "y = df['Survived'].values # y pode ser um vetor\n",
    "\n",
    "# Divide os dados em treino e teste\n",
    "Xtreino, Xteste, Ytreino, Yteste = train_test_split(X, y, test_size = 0.2, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o objeto do tipo SMOTE. O parâmetro k_neighbors indica quantos vizinhos serão considerados para fazer o oversampling.\n",
    "overSampler = SMOTE(k_neighbors = 3, random_state=11)\n",
    "\n",
    "# Aplicando o oversampling \n",
    "Xtreino, Ytreino = overSampler.fit_resample(Xtreino, Ytreino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Countplot da variável alvo no dataset de treino após o oversampling.\n",
    "sns.set_theme(style=\"dark\") # Define o tema utilizado.\n",
    "\n",
    "ax = sns.countplot(x=Ytreino, palette = \"Greens_d\");\n",
    "ax.set_title(\"Frequência absoluta da variável Survived após oversampling\", fontsize = 16)\n",
    "for p in ax.patches: # Exibe os valores no gráfico\n",
    "    _x = p.get_x() + p.get_width() - 0.4\n",
    "    _y = p.get_y() + p.get_height()\n",
    "    value = int(p.get_height())\n",
    "    ax.text(_x, _y, value, ha=\"left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algoritmo</th>\n",
       "      <th>Acurácia</th>\n",
       "      <th>Acertos</th>\n",
       "      <th>Erros</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Regressão Logística</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNN</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Árvore de Decisão</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVM</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bagging Classifier</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Voting Classifier</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Gradient Tree Boosting</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Algoritmo Acurácia Acertos Erros\n",
       "0            Regressão Logística        -       -     -\n",
       "1   Linear Discriminant Analysis        -       -     -\n",
       "2                            KNN        -       -     -\n",
       "3                    Naive Bayes        -       -     -\n",
       "4              Árvore de Decisão        -       -     -\n",
       "5                            SVM        -       -     -\n",
       "6                  Random Forest        -       -     -\n",
       "7             Bagging Classifier        -       -     -\n",
       "8                       AdaBoost        -       -     -\n",
       "9              Voting Classifier        -       -     -\n",
       "10        Gradient Tree Boosting        -       -     -\n",
       "11                 XGBClassifier        -       -     -"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DataFrame para comparar a acurácia de cada algoritmo\n",
    "comparaAlgoritmo = {\"Algoritmo\": [\"Regressão Logística\", \"Linear Discriminant Analysis\", \"KNN\", \"Naive Bayes\", \"Árvore de Decisão\", \"SVM\", \"Random Forest\",\n",
    "                                 \"Bagging Classifier\", \"AdaBoost\", \"Voting Classifier\", \"Gradient Tree Boosting\", \"XGBClassifier\"],\n",
    "                   \"Acurácia\": [\"-\", \"-\", \"-\", \"-\", \"-\", \"-\",\"-\", \"-\", \"-\", \"-\", \"-\", \"-\"],\n",
    "                   \"Acertos\": [\"-\", \"-\", \"-\", \"-\", \"-\", \"-\",\"-\", \"-\", \"-\", \"-\", \"-\", \"-\"],\n",
    "                   \"Erros\": [\"-\", \"-\", \"-\", \"-\", \"-\", \"-\",\"-\", \"-\", \"-\", \"-\", \"-\", \"-\"]}\n",
    "dfComparaAlgoritmo = pd.DataFrame(comparaAlgoritmo)\n",
    "dfComparaAlgoritmo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressão Logística\n",
    "<details>\n",
    "    <summary>\n",
    "        <a class=\"btnfire small stroke\"><em class=\"fas fa-chevron-circle-down\"></em>&nbsp;&nbsp;Clique para mais detalhes</a>\n",
    "    </summary>\n",
    "    <br>\n",
    "\n",
    "O algoritmo Regressão Logística, apesar do nome, implementa um **modelo linear** para classificação em vez de regressão. Neste modelo, as probabilidades que descrevem os possíveis resultados são modeladas usando uma função logística. O algoritmo Regressão Logística implementa classificação binária ou multiclasse utilizando regularização. <br>\n",
    "Regularização é uma técnica de redução de variáveis, onde o coeficiente de cada variável recebe um peso com o objetivo de atribuir níveis de importância. Dessa forma, a depender do método utilizado, as variáveis são classificadas com menor importância ou excluídas do modelo. É muito útil quando o conjunto de dados possui muitas variáveis.<br> \n",
    "A regularização procura reduzir as variáveis colineares. Numa análise de regressão, duas variáveis independentes podem estar altamente correlacionadas, mantendo entre si elevada colinearidade, de tal forma que não é possível estabelecer o efeito de cada uma delas sobre a variável dependente. Variáveis colineares carregam as mesmas informações e aumentam o erro do modelo. <br>\n",
    "Existem três métodos principais de regularização: Ridge Regression, Lasso Regression e Elastic-Net. <br>\n",
    "O método **Ridge Regression**, também conhecido como Regularização L2, aplica o conceito de penalização, ou seja, atribui menor importância a variáveis colineares utilizando um parâmetro chamado de termo de penalidade, que regulariza os coeficientes das variáveis, de tal forma que, se os coeficientes assumem valores grandes, a variável recebe menor importância. <br>\n",
    "O método **LASSO Regression**, conhecido como Regularização L1, também utiliza o conceito de penalização das variáveis colineares, mas considera o valor absoluto dos coeficientes das variáveis para penalizá-las. A penalização acontece diminuindo o valor absoluto dos coeficientes das variáveis até convergirem a zero, o que acaba excluindo a variável do modelo. <br>\n",
    "Já o método **Elastic-Net** é a combinação dos métodos Ridge e LASSO Regression. Essa combinação permite criar um modelo onde alguns dos pesos são diferentes de zero como no método LASSO, mantendo as propriedades de regularização do método Ridge.<br>\n",
    "A regularização é realizada pelo algoritmo de Regressão Logística durante seu treinamento e ajuda a reduzir o overffitng.<br>\n",
    "Cada algoritmo possui **parâmetros** que podem ser ajustados pelo cientista de dados com o objetivo de aumentar o desempenho do modelo. Os parâmetros dos algoritmos de machine learning também são chamados na literatura de **hiperparâmetros**. <br>\n",
    "**Principais parâmetros:**<br>\n",
    "O parâmetro **Penalty** especifica o método utilizado para regularizar os dados. Se nenhum valor for especificado, a regularização padrão é L2. As opções de penalização são:<br>\n",
    "- ‘none’: nenhuma penalidade é adicionada;<br>\n",
    "- ‘l2’: adiciona o termo de penalidade L2 e é a regularização padrão;<br>\n",
    "- ‘l1’: adiciona o termo de penalidade L1; <br>\n",
    "- ‘elasticnet’: os termos de penalidade L1 e L2 são adicionados. <br>\n",
    "\n",
    "Com o parâmetro **Solver**, o cientista de dados informa o algoritmo utilizado na otimização do problema. O padrão é 'lbfgs'. Para escolher um algoritmo, deve-se considerar que para conjuntos de dados pequenos, ‘liblinear’ é uma boa escolha, enquanto ‘sag’ e ‘saga’ são mais rápidos para grandes conjuntos de dados. A escolha do algoritmo depende da penalidade escolhida. Abaixo estão as penalidades suportadas por cada algoritmo:<br>\n",
    "- ‘newton-cg’ - [‘l2’, ‘none’]\n",
    "- ‘lbfgs’ - [‘l2’, ‘none’]\n",
    "- ‘liblinear’ - [‘l1’, ‘l2’]\n",
    "- ‘sag’ - [‘l2’, ‘none’]\n",
    "- ‘saga’ - [‘elasticnet’, ‘l1’, ‘l2’, ‘none’] <br>\n",
    "    \n",
    "O parâmetro **C** define a força da regularização, onde valores menores especificam uma regularização mais forte. Quando nenhum valor é informado, será utilizado o valor padrão igual a 1.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o módulo do algoritmo\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 84.92%\n",
      "Número de acertos: 152\n",
      "Número de erros: 27\n"
     ]
    }
   ],
   "source": [
    "# Criando o modelo\n",
    "regressaoLogistica = LogisticRegression()\n",
    "\n",
    "# Treinamento do modelo\n",
    "regressaoLogistica.fit(Xtreino, Ytreino)\n",
    "\n",
    "# Previsões com os dados de teste\n",
    "previsoes = regressaoLogistica.predict(Xteste)\n",
    "\n",
    "# Calculando a acurácia do modelo\n",
    "numeroAcertos = accuracy_score(Yteste, previsoes, normalize=False)\n",
    "numeroErros = len(Yteste)-numeroAcertos\n",
    "acuracia = numeroAcertos/len(Yteste)\n",
    "\n",
    "print(\"Acurácia: %.2f\" % (acuracia * 100) + \"%\")\n",
    "print(\"Número de acertos:\", numeroAcertos)\n",
    "print(\"Número de erros:\", numeroErros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pesquisando os melhores parâmetros com GridSearchCV\n",
    "<details>\n",
    "    <summary>\n",
    "        <a class=\"btnfire small stroke\"><em class=\"fas fa-chevron-circle-down\"></em>&nbsp;&nbsp;Clique para mais detalhes</a>\n",
    "    </summary>\n",
    "    <br>\n",
    "\n",
    "A função GridSearchCV faz uma busca exaustiva pelos melhores parâmetros. Ou seja, ela combina todas as opções possíveis de parâmetros para verificar quais são os melhores. Isso significa que exige bastante recurso computacional e se o conjunto de dados for muito grande, essa tarefa pode demorar horas ou até mesmo dias.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o módulo do algoritmo\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Esse módulo ignara os avisos.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia média: 80.699%\n",
      "Melhores parâmetros para o modelo: {'C': 10, 'penalty': 'l1', 'solver': 'saga'}\n"
     ]
    }
   ],
   "source": [
    "# Cria um dicionário com os valores que serão testados como parâmetro\n",
    "parametros = {\n",
    "    \"penalty\": ['l1', 'l2', 'elasticnet', 'none'],\n",
    "    \"solver\": ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "    \"C\": [0.001,0.01,0.1,1.0,10,100,1000]\n",
    "}\n",
    "\n",
    "# Cria o modelo que desejamos testar os melhores parâmetros\n",
    "regressaoLogistica = LogisticRegression()\n",
    "\n",
    "# Cria o objeto do tipo GridSearchCV\n",
    "gridSearch = GridSearchCV(estimator = regressaoLogistica, param_grid = parametros)\n",
    "\n",
    "# Treinando os parâmetros. ATENÇÃO: Deve-se usar todo conjunto de dados.\n",
    "gridSearch.fit(X, y)\n",
    "\n",
    "# Print do resultado\n",
    "print(\"Acurácia média: %.3f\" % (gridSearch.best_score_ * 100) + \"%\")\n",
    "print(\"Melhores parâmetros para o modelo:\", gridSearch.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando os melhores parâmetros em uma lista\n",
    "melhoresParametros = []\n",
    "for k in gridSearch.best_params_:\n",
    "    melhoresParametros.append(gridSearch.best_params_[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 86.03%\n",
      "Número de acertos: 154\n",
      "Número de erros: 25\n"
     ]
    }
   ],
   "source": [
    "# Utilizando os melhores parâmetros segundo GridSearchCV\n",
    "# Criando o modelo\n",
    "regressaoLogistica = LogisticRegression(C = melhoresParametros[0], penalty = melhoresParametros[1], solver = melhoresParametros[2])\n",
    "\n",
    "# Treinamento do modelo\n",
    "regressaoLogistica.fit(Xtreino, Ytreino)\n",
    "\n",
    "# Previsões com os dados de teste\n",
    "previsoes = regressaoLogistica.predict(Xteste)\n",
    "\n",
    "# Calculando a acurácia do modelo\n",
    "numeroAcertos = accuracy_score(Yteste, previsoes, normalize=False)\n",
    "numeroErros = len(Yteste)-numeroAcertos\n",
    "acuracia = numeroAcertos/len(Yteste)\n",
    "\n",
    "print(\"Acurácia: %.2f\" % (acuracia * 100) + \"%\")\n",
    "print(\"Número de acertos:\", numeroAcertos)\n",
    "print(\"Número de erros:\", numeroErros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acurácia sem escolher os melhores valores para os parâmetros: 84.92%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pesquisando os melhores parâmetros com RandomizedSearchCV\n",
    "<details>\n",
    "    <summary>\n",
    "        <a class=\"btnfire small stroke\"><em class=\"fas fa-chevron-circle-down\"></em>&nbsp;&nbsp;Clique para mais detalhes</a>\n",
    "    </summary>\n",
    "    <br>\n",
    "\n",
    "Enquanto a função GridSearchCV testa todas as combinações dos possíveis parâmetros, a função RandomizedSearchCV testa de forma aleatória um conjunto de parâmetros. Dessa forma, exige menos recursos computacionais.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o módulo do algoritmo\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia média: 80.586%\n",
      "Melhores parâmetros para o modelo: {'solver': 'lbfgs', 'penalty': 'none', 'C': 100}\n"
     ]
    }
   ],
   "source": [
    "# Cria um dicionário com os valores que serão testados como parâmetro\n",
    "parametros = {\n",
    "    \"penalty\": ['l1', 'l2', 'elasticnet', 'none'],\n",
    "    \"solver\": ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "    \"C\": [0.001,0.01,0.1,1.0,10,100,1000]\n",
    "}\n",
    "\n",
    "# Cria o modelo que desejamos testar os melhores parâmetros\n",
    "regressaoLogistica = LogisticRegression()\n",
    "\n",
    "# Cria o objeto do tipo RandomizedSearchCV\n",
    "randomizedSearch = RandomizedSearchCV(estimator = regressaoLogistica, param_distributions = parametros, random_state = 11)\n",
    "\n",
    "# Treinando os parâmetros. ATENÇÃO: Deve-se usar todo conjunto de dados.\n",
    "randomizedSearch.fit(X, y)\n",
    "\n",
    "# Print do resultado\n",
    "print(\"Acurácia média: %.3f\" % (randomizedSearch.best_score_ * 100) + \"%\")\n",
    "print(\"Melhores parâmetros para o modelo:\", randomizedSearch.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando os melhores parâmetros em uma lista\n",
    "melhoresParametrosLR = []\n",
    "for k in randomizedSearch.best_params_:\n",
    "    melhoresParametrosLR.append(randomizedSearch.best_params_[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 86.03%\n",
      "Número de acertos: 154\n",
      "Número de erros: 25\n"
     ]
    }
   ],
   "source": [
    "# Utilizando os melhores parâmetros segundo o RandomizedSearchCV\n",
    "# Criando o modelo\n",
    "regressaoLogistica = LogisticRegression(solver = melhoresParametrosLR[0], penalty = melhoresParametrosLR[1], C = melhoresParametrosLR[2])\n",
    "\n",
    "# Treinamento do modelo\n",
    "regressaoLogistica.fit(Xtreino, Ytreino)\n",
    "\n",
    "# Previsões com os dados de teste\n",
    "previsoes = regressaoLogistica.predict(Xteste)\n",
    "\n",
    "# Calculando a acurácia do modelo\n",
    "numeroAcertos = accuracy_score(Yteste, previsoes, normalize=False)\n",
    "numeroErros = len(Yteste)-numeroAcertos\n",
    "acuracia = numeroAcertos/len(Yteste)\n",
    "\n",
    "print(\"Acurácia: %.2f\" % (acuracia * 100) + \"%\")\n",
    "print(\"Número de acertos:\", numeroAcertos)\n",
    "print(\"Número de erros:\", numeroErros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acurácia sem escolher os melhores valores para os parâmetros: 84.92%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algoritmo</th>\n",
       "      <th>Acurácia</th>\n",
       "      <th>Acertos</th>\n",
       "      <th>Erros</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Regressão Logística</td>\n",
       "      <td>86.03%</td>\n",
       "      <td>154</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNN</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Árvore de Decisão</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVM</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bagging Classifier</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Voting Classifier</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Gradient Tree Boosting</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Algoritmo Acurácia Acertos Erros\n",
       "0            Regressão Logística   86.03%     154    25\n",
       "1   Linear Discriminant Analysis        -       -     -\n",
       "2                            KNN        -       -     -\n",
       "3                    Naive Bayes        -       -     -\n",
       "4              Árvore de Decisão        -       -     -\n",
       "5                            SVM        -       -     -\n",
       "6                  Random Forest        -       -     -\n",
       "7             Bagging Classifier        -       -     -\n",
       "8                       AdaBoost        -       -     -\n",
       "9              Voting Classifier        -       -     -\n",
       "10        Gradient Tree Boosting        -       -     -\n",
       "11                 XGBClassifier        -       -     -"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Atualizando o dataframe com o desempenho dos algoritmos\n",
    "dfComparaAlgoritmo.iloc[0,1] = \"%.2f\" % (acuracia * 100) + \"%\"\n",
    "dfComparaAlgoritmo.iloc[0,2] = numeroAcertos\n",
    "dfComparaAlgoritmo.iloc[0,3] = numeroErros\n",
    "dfComparaAlgoritmo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis\n",
    "<details>\n",
    "    <summary>\n",
    "        <a class=\"btnfire small stroke\"><em class=\"fas fa-chevron-circle-down\"></em>&nbsp;&nbsp;Clique para mais detalhes</a>\n",
    "    </summary>\n",
    "    <br>\n",
    "\n",
    "Linear Discriminant Analysis é um algoritmo de classificação que cria um **modelo linear**, ajustando densidades condicionais das classes aos dados e usando a regra de Bayes. <br>\n",
    "\n",
    "**Principais parâmetros:**<br>\n",
    "Com o parâmetro **Solver**, difinimos o algoritmo utilizado na otimização do problema. O padrão do parâmetro solver é ‘svd’.  As opções de algoritmos são:<br>\n",
    "- ‘svd’: significa Decomposição de Valor Singular. Essa é a opção padrão, caso nenhum algoritmo seja escolhido. Não calcula a matriz de covariância, portanto este algoritmo é recomendado para dados com um grande número de colunas;<br>\n",
    "- ‘lsqr’: significa Solução de Mínimos Quadrados. Pode ser combinado com o parâmetro Shrinkage ou com o estimador de covariância personalizado;<br>\n",
    "- ‘eigen’: significa Decomposição de Autovalor. Pode ser combinado com o parâmetro Shrinkage ou com o estimador de covariância personalizado;<br>\n",
    "\n",
    "Outro  importante parâmetro é o **Shrinkage**.  Funciona somente com os algoritmos ‘lsqr’ e ‘eigen’. Os possíveis valores desse parâmetro são:<br>\n",
    "- Nenhum valor, que é o padrão;\n",
    "- ‘auto’: usa o padrão Ledoit-Wolf;\n",
    "- valores entre 0 e 1 do tipo float .\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o módulo do algoritmo\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia média: 79.237%\n",
      "Melhores parâmetros para o modelo: {'solver': 'eigen', 'shrinkage': 0.01}\n"
     ]
    }
   ],
   "source": [
    "# Pesquisando os melhores parâmetros com RandomizedSearchCV\n",
    "# Cria um dicionário com os valores que serão testados como parâmetro\n",
    "parametros = {\n",
    "    \"solver\": ['svd', 'lsqr', 'eigen'],\n",
    "    \"shrinkage\": [0.001,0.01,0.1,0.5,0.8,0.2]\n",
    "}\n",
    "\n",
    "# Cria o modelo que desejamos testar os melhores parâmetros\n",
    "linearDiscriminantAnalysis = LinearDiscriminantAnalysis()\n",
    "\n",
    "# Cria o objeto do tipo RandomizedSearchCV\n",
    "randomizedSearch = RandomizedSearchCV(estimator = linearDiscriminantAnalysis, param_distributions = parametros, random_state = 11)\n",
    "\n",
    "# Treinando os parâmetros. ATENÇÃO: Deve-se usar todo conjunto de dados.\n",
    "randomizedSearch.fit(X, y)\n",
    "\n",
    "# Print do resultado\n",
    "print(\"Acurácia média: %.3f\" % (randomizedSearch.best_score_ * 100) + \"%\")\n",
    "print(\"Melhores parâmetros para o modelo:\", randomizedSearch.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando os melhores parâmetros em uma lista\n",
    "melhoresParametros = []\n",
    "for k in randomizedSearch.best_params_:\n",
    "    melhoresParametros.append(randomizedSearch.best_params_[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 85.47%\n",
      "Número de acertos: 153\n",
      "Número de erros: 26\n"
     ]
    }
   ],
   "source": [
    "# Utilizando os melhores parâmetros segundo o RandomizedSearchCV\n",
    "# Criando o modelo\n",
    "linearDiscriminantAnalysis = LinearDiscriminantAnalysis(solver=melhoresParametros[0], shrinkage=melhoresParametros[1])\n",
    "\n",
    "# Treinamento do modelo\n",
    "linearDiscriminantAnalysis.fit(Xtreino, Ytreino)\n",
    "\n",
    "# Previsões com os dados de teste\n",
    "previsoes = linearDiscriminantAnalysis.predict(Xteste)\n",
    "\n",
    "# Calculando a acurácia do modelo\n",
    "numeroAcertos = accuracy_score(Yteste, previsoes, normalize=False)\n",
    "numeroErros = len(Yteste)-numeroAcertos\n",
    "acuracia = numeroAcertos/len(Yteste)\n",
    "\n",
    "print(\"Acurácia: %.2f\" % (acuracia * 100) + \"%\")\n",
    "print(\"Número de acertos:\", numeroAcertos)\n",
    "print(\"Número de erros:\", numeroErros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acurácia sem escolher os melhores valores para os parâmetros: 85.47%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algoritmo</th>\n",
       "      <th>Acurácia</th>\n",
       "      <th>Acertos</th>\n",
       "      <th>Erros</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Regressão Logística</td>\n",
       "      <td>86.03%</td>\n",
       "      <td>154</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>85.47%</td>\n",
       "      <td>153</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNN</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Árvore de Decisão</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVM</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bagging Classifier</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Voting Classifier</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Gradient Tree Boosting</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Algoritmo Acurácia Acertos Erros\n",
       "0            Regressão Logística   86.03%     154    25\n",
       "1   Linear Discriminant Analysis   85.47%     153    26\n",
       "2                            KNN        -       -     -\n",
       "3                    Naive Bayes        -       -     -\n",
       "4              Árvore de Decisão        -       -     -\n",
       "5                            SVM        -       -     -\n",
       "6                  Random Forest        -       -     -\n",
       "7             Bagging Classifier        -       -     -\n",
       "8                       AdaBoost        -       -     -\n",
       "9              Voting Classifier        -       -     -\n",
       "10        Gradient Tree Boosting        -       -     -\n",
       "11                 XGBClassifier        -       -     -"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Atualizando o dataframe com o desempenho dos algoritmos\n",
    "dfComparaAlgoritmo.iloc[1,1] = \"%.2f\" % (acuracia * 100) + \"%\"\n",
    "dfComparaAlgoritmo.iloc[1,2] = numeroAcertos\n",
    "dfComparaAlgoritmo.iloc[1,3] = numeroErros\n",
    "dfComparaAlgoritmo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN - K-Nearest Neighbors\n",
    "<details>\n",
    "    <summary>\n",
    "        <a class=\"btnfire small stroke\"><em class=\"fas fa-chevron-circle-down\"></em>&nbsp;&nbsp;Clique para mais detalhes</a>\n",
    "    </summary>\n",
    "    <br>\n",
    "\n",
    "O KNN cria um modelo **não linear** que considera a distância entre os dados para encontrar padrões no dataset. Normalmente, utiliza-se a distância euclidiana para encontrar semelhanças entre os pontos de dados. <br>\n",
    "É muito importante definir a quantidade de **k**, ou seja, o número de vizinhos. Em geral, um maior número de vizinhos diminui os efeitos do ruído, mas torna os limites de classificação dos grupos menos distintos.<br>\n",
    "Os dados devem estar normalizados e os outliers devem ser tratados, pois influenciam bastante no algoritmo. <br>\n",
    "**Principais parâmetros:**<br>\n",
    "O parâmetro **n_neighbors** define o número de vizinhos. Seu valor padrão é 5.<br>\n",
    "Com o parâmetro **weights** definimos pesos aos vizinhos. Possíveis valores:<br>\n",
    "- uniform: todos os pontos de dados recebem o mesmo peso;<br>\n",
    "- distance: o peso é definido pela distância entre os dados. Neste caso, os vizinhos mais próximos de um ponto de dado terão uma influência maior do que os vizinhos mais distantes.<br>\n",
    "\n",
    "O KNN utiliza alguns algoritmos para calcular os vizinhos mais próximos e o cientista de dados pode testar o melhor definindo o parâmetro **algorithm**:<br>\n",
    "- ball_tree;<br>\n",
    "- kd_tree;<br>\n",
    "- brute;<br>\n",
    "- auto: com essa opção o KNN decidirá qual é o algoritmo mais apropriado baseado nos valores dos dados.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o módulo do algoritmo\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia média: 80.476%\n",
      "Melhores parâmetros para o modelo: {'weights': 'uniform', 'n_neighbors': 4, 'algorithm': 'brute'}\n"
     ]
    }
   ],
   "source": [
    "# Pesquisando os melhores parâmetros com RandomizedSearchCV\n",
    "# Cria um dicionário com os valores que serão testados como parâmetro\n",
    "parametros = {\n",
    "    \"n_neighbors\": [2, 3, 4, 5, 6, 7, 8, 9],\n",
    "    \"weights\": ['uniform', 'distance'],\n",
    "    \"algorithm\": ['ball_tree', 'kd_tree', 'brute']\n",
    "}\n",
    "\n",
    "# Cria o modelo que desejamos testar os melhores parâmetros\n",
    "kNeighborsClassifier = KNeighborsClassifier()\n",
    "\n",
    "# Cria o objeto do tipo RandomizedSearchCV\n",
    "randomizedSearch = RandomizedSearchCV(estimator = kNeighborsClassifier, param_distributions = parametros, random_state = 11)\n",
    "\n",
    "# Treinando os parâmetros. ATENÇÃO: Deve-se usar todo conjunto de dados.\n",
    "randomizedSearch.fit(X, y)\n",
    "\n",
    "# Print do resultado\n",
    "print(\"Acurácia média: %.3f\" % (randomizedSearch.best_score_ * 100) + \"%\")\n",
    "print(\"Melhores parâmetros para o modelo:\", randomizedSearch.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando os melhores parâmetros em uma lista\n",
    "melhoresParametros = []\n",
    "for k in randomizedSearch.best_params_:\n",
    "    melhoresParametros.append(randomizedSearch.best_params_[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 87.71%\n",
      "Número de acertos: 157\n",
      "Número de erros: 22\n"
     ]
    }
   ],
   "source": [
    "# Criando o modelo\n",
    "kNeighborsClassifier = KNeighborsClassifier(weights = melhoresParametros[0], n_neighbors = melhoresParametros[1], algorithm = melhoresParametros[2])\n",
    "\n",
    "# Treinamento do modelo\n",
    "kNeighborsClassifier.fit(Xtreino, Ytreino)\n",
    "\n",
    "# Previsões com os dados de teste\n",
    "previsoes = kNeighborsClassifier.predict(Xteste)\n",
    "\n",
    "# Calculando a acurácia do modelo\n",
    "numeroAcertos = accuracy_score(Yteste, previsoes, normalize=False)\n",
    "numeroErros = len(Yteste)-numeroAcertos\n",
    "acuracia = numeroAcertos/len(Yteste)\n",
    "\n",
    "print(\"Acurácia: %.2f\" % (acuracia * 100) + \"%\")\n",
    "print(\"Número de acertos:\", numeroAcertos)\n",
    "print(\"Número de erros:\", numeroErros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acurácia sem escolher os melhores valores para os parâmetros: 82.68%\n",
    "# Acurácia sem normalizar os dados: 74.30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algoritmo</th>\n",
       "      <th>Acurácia</th>\n",
       "      <th>Acertos</th>\n",
       "      <th>Erros</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Regressão Logística</td>\n",
       "      <td>86.03%</td>\n",
       "      <td>154</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>85.47%</td>\n",
       "      <td>153</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNN</td>\n",
       "      <td>87.71%</td>\n",
       "      <td>157</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Árvore de Decisão</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVM</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bagging Classifier</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Voting Classifier</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Gradient Tree Boosting</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Algoritmo Acurácia Acertos Erros\n",
       "0            Regressão Logística   86.03%     154    25\n",
       "1   Linear Discriminant Analysis   85.47%     153    26\n",
       "2                            KNN   87.71%     157    22\n",
       "3                    Naive Bayes        -       -     -\n",
       "4              Árvore de Decisão        -       -     -\n",
       "5                            SVM        -       -     -\n",
       "6                  Random Forest        -       -     -\n",
       "7             Bagging Classifier        -       -     -\n",
       "8                       AdaBoost        -       -     -\n",
       "9              Voting Classifier        -       -     -\n",
       "10        Gradient Tree Boosting        -       -     -\n",
       "11                 XGBClassifier        -       -     -"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Atualizando o dataframe com o desempenho dos algoritmos\n",
    "dfComparaAlgoritmo.iloc[2,1] = \"%.2f\" % (acuracia * 100) + \"%\"\n",
    "dfComparaAlgoritmo.iloc[2,2] = numeroAcertos\n",
    "dfComparaAlgoritmo.iloc[2,3] = numeroErros\n",
    "dfComparaAlgoritmo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "<details>\n",
    "    <summary>\n",
    "        <a class=\"btnfire small stroke\"><em class=\"fas fa-chevron-circle-down\"></em>&nbsp;&nbsp;Clique para mais detalhes</a>\n",
    "    </summary>\n",
    "    <br>\n",
    "\n",
    "Naive Bayes é um algoritmo **não-linear** e **probabilístico**, ou seja, calcula a probabilidade de ocorrência e a probabilidade condicional de cada classe. Assume que os dados estão em uma distribuição Gaussiana, ou seja em uma distribuição Normal, com média igual a zero e desvio padrão igual a 1. <br>\n",
    "O Naive Bayes possui apenas dois parâmetros e por essa razão, exige uma boa preparação dos dados. Normalmente, não definimos valores para os parâmetros e deixamos o algoritmo encontrar os melhores valores.<br>\n",
    "Excelente para previsões em tempo real e muito útil para dados no formato de texto.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o módulo do algoritmo\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 83.24%\n",
      "Número de acertos: 149\n",
      "Número de erros: 30\n"
     ]
    }
   ],
   "source": [
    "# Criando o modelo\n",
    "gaussianNB = GaussianNB()\n",
    "\n",
    "# Treinamento do modelo\n",
    "gaussianNB.fit(Xtreino, Ytreino)\n",
    "\n",
    "# Previsões com os dados de teste\n",
    "previsoes = gaussianNB.predict(Xteste)\n",
    "\n",
    "# Calculando a acurácia do modelo\n",
    "numeroAcertos = accuracy_score(Yteste, previsoes, normalize=False)\n",
    "numeroErros = len(Yteste)-numeroAcertos\n",
    "acuracia = numeroAcertos/len(Yteste)\n",
    "\n",
    "print(\"Acurácia: %.2f\" % (acuracia * 100) + \"%\")\n",
    "print(\"Número de acertos:\", numeroAcertos)\n",
    "print(\"Número de erros:\", numeroErros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algoritmo</th>\n",
       "      <th>Acurácia</th>\n",
       "      <th>Acertos</th>\n",
       "      <th>Erros</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Regressão Logística</td>\n",
       "      <td>86.03%</td>\n",
       "      <td>154</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>85.47%</td>\n",
       "      <td>153</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNN</td>\n",
       "      <td>87.71%</td>\n",
       "      <td>157</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>83.24%</td>\n",
       "      <td>149</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Árvore de Decisão</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVM</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bagging Classifier</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Voting Classifier</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Gradient Tree Boosting</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Algoritmo Acurácia Acertos Erros\n",
       "0            Regressão Logística   86.03%     154    25\n",
       "1   Linear Discriminant Analysis   85.47%     153    26\n",
       "2                            KNN   87.71%     157    22\n",
       "3                    Naive Bayes   83.24%     149    30\n",
       "4              Árvore de Decisão        -       -     -\n",
       "5                            SVM        -       -     -\n",
       "6                  Random Forest        -       -     -\n",
       "7             Bagging Classifier        -       -     -\n",
       "8                       AdaBoost        -       -     -\n",
       "9              Voting Classifier        -       -     -\n",
       "10        Gradient Tree Boosting        -       -     -\n",
       "11                 XGBClassifier        -       -     -"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Atualizando o dataframe com o desempenho dos algoritmos\n",
    "dfComparaAlgoritmo.iloc[3,1] = \"%.2f\" % (acuracia * 100) + \"%\"\n",
    "dfComparaAlgoritmo.iloc[3,2] = numeroAcertos\n",
    "dfComparaAlgoritmo.iloc[3,3] = numeroErros\n",
    "dfComparaAlgoritmo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Árvores de Decisão\n",
    "<details>\n",
    "    <summary>\n",
    "        <a class=\"btnfire small stroke\"><em class=\"fas fa-chevron-circle-down\"></em>&nbsp;&nbsp;Clique para mais detalhes</a>\n",
    "    </summary>\n",
    "    <br>\n",
    "\n",
    "O algoritmo de árvore de decisão utiliza métodos baseados em procura para criar modelos **não-lineares estruturados em árvore de decisão**. A árvore é uma segmentação supervisionada, porque cada folha contém um valor para a variável alvo. Em um projeto supervisionado de classificação, cada folha contém uma classificação para seu segmento. Tal árvore é chamada de árvore de classificação ou, mais livremente, árvore de decisão. <br>\n",
    "Não exige muita preparação dos dados e é indiferente quanto à normalização e à padronização. <br>\n",
    "O algoritmo de árvore de decisão pode criar árvores super complexas que não generalizam bem os dados. Esse problema é chamado de overfitting. Uma solução para o overfitting gerado pela árvore de decisão é realizar a poda da árvore, que em inglês é conhecida como pruning. Realizar a poda significa eliminar alguns dos ramos inferiores da árvore, de forma a atenuar o efeito dos ruídos, que são informações inúteis para a classificação, de forma a manter na árvore apenas regras com maior poder de distinguir as classes.<br>\n",
    "Para realizar a poda podemos fazer ajustes nos parâmetros do algoritmo. A poda pode ser feita definindo o número mínimo de amostras necessárias em um nó folha com o parâmetro **min_samples_split** ou definindo a profundidade máxima da árvore com o parâmetro **max_depth**. \n",
    "Outros importantes parâmetros: <br>\n",
    "Com o parâmetro **criterion** podemos medir a qualidade dos nós da árvore. Os valores suportados por este parâmetro são:<br>\n",
    "- gini para a impureza; <br>\n",
    "- log_loss e entropia, ambos para o ganho de informação. <br>\n",
    "    \n",
    "O parâmetro **splitter** define a estratégia usada para escolher a divisão em cada nó. As estratégias suportadas são: <br>\n",
    "- best: para escolher a melhor divisão; e <br>\n",
    "- random: para escolher a melhor divisão aleatória.<br>\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o módulo do algoritmo\n",
    "from sklearn.tree import DecisionTreeClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia média: 81.933%\n",
      "Melhores parâmetros para o modelo: {'splitter': 'random', 'min_samples_split': 6, 'max_depth': 550, 'criterion': 'log_loss'}\n"
     ]
    }
   ],
   "source": [
    "# Pesquisando os melhores parâmetros com RandomizedSearchCV\n",
    "# Cria um dicionário com os valores que serão testados como parâmetro\n",
    "parametros = {\n",
    "    \"min_samples_split\": [2, 3, 4, 5, 6],\n",
    "    \"max_depth\": [100, 150, 200, 350, 400, 550],\n",
    "    \"criterion\": ['gini', 'log_loss', 'entropia'],\n",
    "    \"splitter\": [\"best\", \"random\"]\n",
    "}\n",
    "\n",
    "# Cria o modelo que desejamos testar os melhores parâmetros\n",
    "decisionTreeClassifier = DecisionTreeClassifier(random_state=81)\n",
    "\n",
    "# Cria o objeto do tipo RandomizedSearchCV\n",
    "randomizedSearch = RandomizedSearchCV(estimator = decisionTreeClassifier, param_distributions = parametros, random_state = 11)\n",
    "\n",
    "# Treinando os parâmetros. ATENÇÃO: Deve-se usar todo conjunto de dados.\n",
    "randomizedSearch.fit(X, y)\n",
    "\n",
    "# Print do resultado\n",
    "print(\"Acurácia média: %.3f\" % (randomizedSearch.best_score_ * 100) + \"%\")\n",
    "print(\"Melhores parâmetros para o modelo:\", randomizedSearch.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando os melhores parâmetros em uma lista\n",
    "melhoresParametrosAD = []\n",
    "for k in randomizedSearch.best_params_:\n",
    "    melhoresParametrosAD.append(randomizedSearch.best_params_[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 86.03%\n",
      "Número de acertos: 154\n",
      "Número de erros: 25\n"
     ]
    }
   ],
   "source": [
    "# Criando o modelo\n",
    "decisionTreeClassifier = DecisionTreeClassifier(splitter = melhoresParametrosAD[0], \n",
    "                                     min_samples_split = melhoresParametrosAD[1],\n",
    "                                     max_depth = melhoresParametrosAD[2],\n",
    "                                     criterion = melhoresParametrosAD[3],\n",
    "                                     random_state=50)\n",
    "\n",
    "# Treinamento do modelo\n",
    "decisionTreeClassifier.fit(Xtreino, Ytreino)\n",
    "\n",
    "# Previsões com os dados de teste\n",
    "previsoes = decisionTreeClassifier.predict(Xteste)\n",
    "\n",
    "# Calculando a acurácia do modelo\n",
    "numeroAcertos = accuracy_score(Yteste, previsoes, normalize=False)\n",
    "numeroErros = len(Yteste)-numeroAcertos\n",
    "acuracia = numeroAcertos/len(Yteste)\n",
    "\n",
    "print(\"Acurácia: %.2f\" % (acuracia * 100) + \"%\")\n",
    "print(\"Número de acertos:\", numeroAcertos)\n",
    "print(\"Número de erros:\", numeroErros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acurácia sem escolher os melhores valores para os parâmetros: 79.33%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algoritmo</th>\n",
       "      <th>Acurácia</th>\n",
       "      <th>Acertos</th>\n",
       "      <th>Erros</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Regressão Logística</td>\n",
       "      <td>86.03%</td>\n",
       "      <td>154</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>85.47%</td>\n",
       "      <td>153</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNN</td>\n",
       "      <td>87.71%</td>\n",
       "      <td>157</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>83.24%</td>\n",
       "      <td>149</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Árvore de Decisão</td>\n",
       "      <td>86.03%</td>\n",
       "      <td>154</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVM</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bagging Classifier</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Voting Classifier</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Gradient Tree Boosting</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Algoritmo Acurácia Acertos Erros\n",
       "0            Regressão Logística   86.03%     154    25\n",
       "1   Linear Discriminant Analysis   85.47%     153    26\n",
       "2                            KNN   87.71%     157    22\n",
       "3                    Naive Bayes   83.24%     149    30\n",
       "4              Árvore de Decisão   86.03%     154    25\n",
       "5                            SVM        -       -     -\n",
       "6                  Random Forest        -       -     -\n",
       "7             Bagging Classifier        -       -     -\n",
       "8                       AdaBoost        -       -     -\n",
       "9              Voting Classifier        -       -     -\n",
       "10        Gradient Tree Boosting        -       -     -\n",
       "11                 XGBClassifier        -       -     -"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Atualizando o dataframe com o desempenho dos algoritmos\n",
    "dfComparaAlgoritmo.iloc[4,1] = \"%.2f\" % (acuracia * 100) + \"%\"\n",
    "dfComparaAlgoritmo.iloc[4,2] = numeroAcertos\n",
    "dfComparaAlgoritmo.iloc[4,3] = numeroErros\n",
    "dfComparaAlgoritmo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM - Support Vector Machines\n",
    "<details>\n",
    "    <summary>\n",
    "        <a class=\"btnfire small stroke\"><em class=\"fas fa-chevron-circle-down\"></em>&nbsp;&nbsp;Clique para mais detalhes</a>\n",
    "    </summary>\n",
    "    <br>\n",
    "\n",
    "O objetivo do algoritmo SVM é encontrar o hiperplano, que é responsável por dividir os dados entre suas respectivas classes. Para calcular o hiperplano, o algoritmo precisa encontrar as margens, que também são chamadas de vetores de suporte. Os vetores de suporte definem os limites de cada classe. O hiperplano deve estar exatamente ao centro entre os vetores, ou seja, a distância entre o hiperplano e cada vetor de suporte deve ser a mesma. O hiperplano e os vetores de suporte são calculados durante o treinamento do modelo. <br>\n",
    "O algoritmo é muito sensível a outliers, visto que, o outlier afeta diretamente o cálculo da margem de cada classe. Logo, exige grande preparação dos dados, visto que, os ruídos podem prejudicar a definição dos vetores. Funciona muito bem com dados, onde existe uma clara margem de separação dos dados. <br>\n",
    "Consegue classificar os dados linearmente separáveis, como também aqueles que não são linearmente separáveis. Quando os dados não são linearmente separáveis o algoritmo SVM eleva os dados a uma terceira dimensão, ou a uma dimensão superior, onde consegue traçar o hiperplano. De fato, ao elevar os dados a uma dimensão superior, o algoritmo SVM oferece uma separação linear dos dados.<br>\n",
    "Principais parâmetros:<br>\n",
    "Com o parâmetro **C** definimos a regularização Ridge Regression do algoritmo e deve ser um valor positivo. <br>\n",
    "O parâmetro **kernel** especifica o tipo de kernel que será utilizado pelo algoritmo. As opções são linear, poly, rbf e sigmoid. <br> \n",
    "Outro parâmetro bastante importante é o **decision_function_shape** com as opções ovo e ovr.\n",
    "\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o módulo do algoritmo\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia média: 80.585%\n",
      "Melhores parâmetros para o modelo: {'kernel': 'poly', 'decision_function_shape': 'ovr', 'C': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Pesquisando os melhores parâmetros com RandomizedSearchCV\n",
    "# Cria um dicionário com os valores que serão testados como parâmetro\n",
    "parametros = {\n",
    "   \"C\": [0.001,0.01,0.1,1.0],\n",
    "   \"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n",
    "   \"decision_function_shape\": [\"ovo\", \"ovr\"]\n",
    "}\n",
    "\n",
    "# Cria o modelo que desejamos testar os melhores parâmetros\n",
    "svc = SVC()\n",
    "\n",
    "# Cria o objeto do tipo RandomizedSearchCV\n",
    "randomizedSearch = RandomizedSearchCV(estimator = svc, param_distributions = parametros, random_state = 11)\n",
    "\n",
    "# Treinando os parâmetros. ATENÇÃO: Deve-se usar todo conjunto de dados.\n",
    "randomizedSearch.fit(X, y)\n",
    "\n",
    "# Print do resultado\n",
    "print(\"Acurácia média: %.3f\" % (randomizedSearch.best_score_ * 100) + \"%\")\n",
    "print(\"Melhores parâmetros para o modelo:\", randomizedSearch.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando os melhores parâmetros em uma lista\n",
    "melhoresParametrosSVM = []\n",
    "for k in randomizedSearch.best_params_:\n",
    "    melhoresParametrosSVM.append(randomizedSearch.best_params_[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 87.15%\n",
      "Número de acertos: 156\n",
      "Número de erros: 23\n"
     ]
    }
   ],
   "source": [
    "# Criando o modelo\n",
    "svc = SVC(kernel = melhoresParametrosSVM[0], decision_function_shape = melhoresParametrosSVM[1], C = melhoresParametrosSVM[2])\n",
    "\n",
    "# Treinamento do modelo\n",
    "svc.fit(Xtreino, Ytreino)\n",
    "\n",
    "# Previsões com os dados de teste\n",
    "previsoes = svc.predict(Xteste)\n",
    "\n",
    "# Calculando a acurácia do modelo\n",
    "numeroAcertos = accuracy_score(Yteste, previsoes, normalize=False)\n",
    "numeroErros = len(Yteste)-numeroAcertos\n",
    "acuracia = numeroAcertos/len(Yteste)\n",
    "\n",
    "print(\"Acurácia: %.2f\" % (acuracia * 100) + \"%\")\n",
    "print(\"Número de acertos:\", numeroAcertos)\n",
    "print(\"Número de erros:\", numeroErros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acurácia sem escolher os melhores valores para os parâmetros: 84.92%\n",
    "# Acurácia sem normalizar os dados: 69.97%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algoritmo</th>\n",
       "      <th>Acurácia</th>\n",
       "      <th>Acertos</th>\n",
       "      <th>Erros</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Regressão Logística</td>\n",
       "      <td>86.03%</td>\n",
       "      <td>154</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>85.47%</td>\n",
       "      <td>153</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNN</td>\n",
       "      <td>87.71%</td>\n",
       "      <td>157</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>83.24%</td>\n",
       "      <td>149</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Árvore de Decisão</td>\n",
       "      <td>86.03%</td>\n",
       "      <td>154</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVM</td>\n",
       "      <td>87.15%</td>\n",
       "      <td>156</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bagging Classifier</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Voting Classifier</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Gradient Tree Boosting</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Algoritmo Acurácia Acertos Erros\n",
       "0            Regressão Logística   86.03%     154    25\n",
       "1   Linear Discriminant Analysis   85.47%     153    26\n",
       "2                            KNN   87.71%     157    22\n",
       "3                    Naive Bayes   83.24%     149    30\n",
       "4              Árvore de Decisão   86.03%     154    25\n",
       "5                            SVM   87.15%     156    23\n",
       "6                  Random Forest        -       -     -\n",
       "7             Bagging Classifier        -       -     -\n",
       "8                       AdaBoost        -       -     -\n",
       "9              Voting Classifier        -       -     -\n",
       "10        Gradient Tree Boosting        -       -     -\n",
       "11                 XGBClassifier        -       -     -"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Atualizando o dataframe com o desempenho dos algoritmos\n",
    "dfComparaAlgoritmo.iloc[5,1] = \"%.2f\" % (acuracia * 100) + \"%\"\n",
    "dfComparaAlgoritmo.iloc[5,2] = numeroAcertos\n",
    "dfComparaAlgoritmo.iloc[5,3] = numeroErros\n",
    "dfComparaAlgoritmo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métodos Ensemble\n",
    "<details>\n",
    "    <summary>\n",
    "        <a class=\"btnfire small stroke\"><em class=\"fas fa-chevron-circle-down\"></em>&nbsp;&nbsp;Clique para mais detalhes</a>\n",
    "    </summary>\n",
    "    <br>\n",
    "\n",
    "Método ensemble é uma técnica de aprendizado de máquina que combina o resultado de múltiplos modelos com o objetivo de produzir um melhor modelo preditivo. \n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "<details>\n",
    "    <summary>\n",
    "        <a class=\"btnfire small stroke\"><em class=\"fas fa-chevron-circle-down\"></em>&nbsp;&nbsp;Clique para mais detalhes</a>\n",
    "    </summary>\n",
    "    <br>\n",
    "\n",
    "Random Forest cria uma floresta aleatória com muitas árvores de decisão. O Random Forest seleciona algumas amostras dos dados, de maneira aleatória, por meio do Bootstrap, um método de reamostragem que permite dados repetidos na seleção. Para criar os nós das árvores, também existe uma etapa aleatória, na qual algumas variáveis são selecionadas de forma randômica. Por fim, a combinação é obtida votando no melhor entre os modelos, caso seja um problema de classificação, ou pela média dos modelos, caso seja um problema de regressão. Em contraste com a proposta original do algoritmo, a implementação do scikit-learn combina os modelos pela média de sua previsão probabilística, ao invés da votação em problemas de classificação. <br>\n",
    "Principais parâmetros: <br>\n",
    "**n_estimators**: define o número de árvores da floresta. O valor padrão é 100;<br>\n",
    "**criterion**: função utilizada para medir a qualidade de uma divisão. Os possíveis valores são: gini, entropy, log_loss;<br>\n",
    "**max_depth**: define a profundidade máxima da árvore. Se nenhum valor é informado, os nós são expandidos até que todas as folhas sejam puras ou até que todas as folhas contenham uma  quantidade  de amostras menor do que foi definida no parâmetro min_samples_split.<br>\n",
    "**min_samples_split**: define o número mínimo de amostras necessárias para dividir um nó interno.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o módulo do algoritmo\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia média: 84.178%\n",
      "Melhores parâmetros para o modelo: {'n_estimators': 400, 'min_samples_split': 5, 'max_depth': 400, 'criterion': 'log_loss'}\n"
     ]
    }
   ],
   "source": [
    "# Pesquisando os melhores parâmetros com RandomizedSearchCV\n",
    "# Cria um dicionário com os valores que serão testados como parâmetro\n",
    "parametros = {\n",
    "    \"n_estimators\": [100, 150, 250, 300, 400, 550],\n",
    "    \"criterion\": ['gini', 'log_loss', 'entropia'], \n",
    "    \"max_depth\": [100, 150, 200, 350, 400, 550],\n",
    "    \"min_samples_split\": [2, 3, 4, 5, 6] \n",
    "}\n",
    "\n",
    "# Cria o modelo que desejamos testar os melhores parâmetros\n",
    "randomForestClassifier = RandomForestClassifier(random_state=81)\n",
    "\n",
    "# Cria o objeto do tipo RandomizedSearchCV\n",
    "randomizedSearch = RandomizedSearchCV(estimator = randomForestClassifier, param_distributions = parametros, random_state = 11)\n",
    "\n",
    "# Treinando os parâmetros. ATENÇÃO: Deve-se usar todo conjunto de dados.\n",
    "randomizedSearch.fit(X, y)\n",
    "\n",
    "# Print do resultado\n",
    "print(\"Acurácia média: %.3f\" % (randomizedSearch.best_score_ * 100) + \"%\")\n",
    "print(\"Melhores parâmetros para o modelo:\", randomizedSearch.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando os melhores parâmetros em uma lista\n",
    "melhoresParametros = []\n",
    "for k in randomizedSearch.best_params_:\n",
    "    melhoresParametros.append(randomizedSearch.best_params_[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 87.15%\n",
      "Número de acertos: 156\n",
      "Número de erros: 23\n"
     ]
    }
   ],
   "source": [
    "# Criando o modelo\n",
    "randomForestClassifier = RandomForestClassifier(n_estimators = melhoresParametros[0], \n",
    "                                min_samples_split = melhoresParametros[1], \n",
    "                                max_depth = melhoresParametros[2],\n",
    "                                criterion = melhoresParametros[3],\n",
    "                                random_state=92)\n",
    "\n",
    "# Treinamento do modelo\n",
    "randomForestClassifier.fit(Xtreino, Ytreino)\n",
    "\n",
    "# Previsões com os dados de teste\n",
    "previsoes = randomForestClassifier.predict(Xteste)\n",
    "\n",
    "# Calculando a acurácia do modelo\n",
    "numeroAcertos = accuracy_score(Yteste, previsoes, normalize=False)\n",
    "numeroErros = len(Yteste)-numeroAcertos\n",
    "acuracia = numeroAcertos/len(Yteste)\n",
    "\n",
    "print(\"Acurácia: %.2f\" % (acuracia * 100) + \"%\")\n",
    "print(\"Número de acertos:\", numeroAcertos)\n",
    "print(\"Número de erros:\", numeroErros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acurácia sem escolher os melhores valores para os parâmetros: 86.59%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algoritmo</th>\n",
       "      <th>Acurácia</th>\n",
       "      <th>Acertos</th>\n",
       "      <th>Erros</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Regressão Logística</td>\n",
       "      <td>86.03%</td>\n",
       "      <td>154</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>85.47%</td>\n",
       "      <td>153</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNN</td>\n",
       "      <td>87.71%</td>\n",
       "      <td>157</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>83.24%</td>\n",
       "      <td>149</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Árvore de Decisão</td>\n",
       "      <td>86.03%</td>\n",
       "      <td>154</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVM</td>\n",
       "      <td>87.15%</td>\n",
       "      <td>156</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>87.15%</td>\n",
       "      <td>156</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bagging Classifier</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Voting Classifier</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Gradient Tree Boosting</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Algoritmo Acurácia Acertos Erros\n",
       "0            Regressão Logística   86.03%     154    25\n",
       "1   Linear Discriminant Analysis   85.47%     153    26\n",
       "2                            KNN   87.71%     157    22\n",
       "3                    Naive Bayes   83.24%     149    30\n",
       "4              Árvore de Decisão   86.03%     154    25\n",
       "5                            SVM   87.15%     156    23\n",
       "6                  Random Forest   87.15%     156    23\n",
       "7             Bagging Classifier        -       -     -\n",
       "8                       AdaBoost        -       -     -\n",
       "9              Voting Classifier        -       -     -\n",
       "10        Gradient Tree Boosting        -       -     -\n",
       "11                 XGBClassifier        -       -     -"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Atualizando o dataframe com o desempenho dos algoritmos\n",
    "dfComparaAlgoritmo.iloc[6,1] = \"%.2f\" % (acuracia * 100) + \"%\"\n",
    "dfComparaAlgoritmo.iloc[6,2] = numeroAcertos\n",
    "dfComparaAlgoritmo.iloc[6,3] = numeroErros\n",
    "dfComparaAlgoritmo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging Classifier\n",
    "<details>\n",
    "    <summary>\n",
    "        <a class=\"btnfire small stroke\"><em class=\"fas fa-chevron-circle-down\"></em>&nbsp;&nbsp;Clique para mais detalhes</a>\n",
    "    </summary>\n",
    "    <br>\n",
    "\n",
    "Bagging Classifier é um algoritmo Ensemble que combina o resultado de múltiplos modelos de um mesmo algoritmo, com base na média ou por votação, com o objetivo de produzir um melhor modelo preditivo. Esse algoritmo normalmente é usado como uma forma de reduzir a variância de um modelo base, por exemplo, uma árvore de decisão, introduzindo a aleatoriedade em seu procedimento de construção, e em seguida, construindo um modelo mais preciso. <br>\n",
    "Nós trabalhamos diferente com o algoritmo Bagging Classifier, visto que, não alteramos os seus parâmetros. De fato, utilizamos os melhores parâmetros escolhidos para o modelo base. \n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o módulo do algoritmo\n",
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 87.71%\n",
      "Número de acertos: 157\n",
      "Número de erros: 22\n"
     ]
    }
   ],
   "source": [
    "# Cria o modelo de árvore de decisão\n",
    "decisionTreeClassifierVT = DecisionTreeClassifier(splitter = melhoresParametrosAD[0], \n",
    "                                     min_samples_split = melhoresParametrosAD[1],\n",
    "                                     max_depth = melhoresParametrosAD[2],\n",
    "                                     criterion = melhoresParametrosAD[3],\n",
    "                                     random_state=50)\n",
    "\n",
    "# Criando o modelo Bagging Classifier\n",
    "baggingClassifier = BaggingClassifier(base_estimator = decisionTreeClassifierVT, random_state = 1)\n",
    "\n",
    "# Treinamento do modelo\n",
    "baggingClassifier.fit(Xtreino, Ytreino)\n",
    "\n",
    "# Previsões com os dados de teste\n",
    "previsoes = baggingClassifier.predict(Xteste)\n",
    "\n",
    "# Calculando a acurácia do modelo\n",
    "numeroAcertos = accuracy_score(Yteste, previsoes, normalize=False)\n",
    "numeroErros = len(Yteste)-numeroAcertos\n",
    "acuracia = numeroAcertos/len(Yteste)\n",
    "\n",
    "print(\"Acurácia: %.2f\" % (acuracia * 100) + \"%\")\n",
    "print(\"Número de acertos:\", numeroAcertos)\n",
    "print(\"Número de erros:\", numeroErros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acurácia do modelo Árvore de Decisão sem utilizar o Bagging Classifier: 83,80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algoritmo</th>\n",
       "      <th>Acurácia</th>\n",
       "      <th>Acertos</th>\n",
       "      <th>Erros</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Regressão Logística</td>\n",
       "      <td>86.03%</td>\n",
       "      <td>154</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>85.47%</td>\n",
       "      <td>153</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNN</td>\n",
       "      <td>87.71%</td>\n",
       "      <td>157</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>83.24%</td>\n",
       "      <td>149</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Árvore de Decisão</td>\n",
       "      <td>86.03%</td>\n",
       "      <td>154</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVM</td>\n",
       "      <td>87.15%</td>\n",
       "      <td>156</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>87.15%</td>\n",
       "      <td>156</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bagging Classifier</td>\n",
       "      <td>87.71%</td>\n",
       "      <td>157</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Voting Classifier</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Gradient Tree Boosting</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Algoritmo Acurácia Acertos Erros\n",
       "0            Regressão Logística   86.03%     154    25\n",
       "1   Linear Discriminant Analysis   85.47%     153    26\n",
       "2                            KNN   87.71%     157    22\n",
       "3                    Naive Bayes   83.24%     149    30\n",
       "4              Árvore de Decisão   86.03%     154    25\n",
       "5                            SVM   87.15%     156    23\n",
       "6                  Random Forest   87.15%     156    23\n",
       "7             Bagging Classifier   87.71%     157    22\n",
       "8                       AdaBoost        -       -     -\n",
       "9              Voting Classifier        -       -     -\n",
       "10        Gradient Tree Boosting        -       -     -\n",
       "11                 XGBClassifier        -       -     -"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Atualizando o dataframe com o desempenho dos algoritmos\n",
    "dfComparaAlgoritmo.iloc[7,1] = \"%.2f\" % (acuracia * 100) + \"%\"\n",
    "dfComparaAlgoritmo.iloc[7,2] = numeroAcertos\n",
    "dfComparaAlgoritmo.iloc[7,3] = numeroErros\n",
    "dfComparaAlgoritmo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost\n",
    "<details>\n",
    "    <summary>\n",
    "        <a class=\"btnfire small stroke\"><em class=\"fas fa-chevron-circle-down\"></em>&nbsp;&nbsp;Clique para mais detalhes</a>\n",
    "    </summary>\n",
    "    <br>\n",
    "\n",
    "O algoritmo AdaBoost treina uma sequência de algoritmos fracos em repetidas amostras do conjunto de dados. Em problemas de regressão, as previsões de todos estes modelos são então combinadas por meio de uma soma ponderada para produzir a previsão final. AdaBoost utiliza a técnica Boosting. Essa técnica funciona da seguinte forma, inicialmente, todos os dados do conjunto de treinamento recebem um peso idêntico. Então, o modelo é treinado usando o conjunto de treinamento. Em seguida, o erro do modelo no conjunto de treinamento é calculado, com base em quantos objetos foram classificados de forma correta e quantos foram classificados incorretamente. Na sequência, os pesos são atualizados com base nos erros do modelo. Sendo assim, um novo modelo é treinado usando o conjunto de pesos modificados. Novamente, o erro é calculado, novos pesos são atribuídos e mais uma vez um modelo é treinado. E o processo se repete até o número de iterações inicialmente definido. <br>\n",
    "Principais parâmetros: <br>\n",
    "- **n_estimator**: define o máximo de modelos utilizados pelo Boosting do algoritmo;<br>\n",
    "- **learning_rate**: define o peso aplicado para cada modelo de cada iteração do Boosting. Um valor alto aumenta a contribuição de cada modelo;<br>\n",
    "- **algorithm**: define o algoritmo para calcular o boosting.\n",
    "\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o módulo do algoritmo\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia média: 82.723%\n",
      "Melhores parâmetros para o modelo: {'n_estimators': 100, 'learning_rate': 1.0, 'algorithm': 'SAMME.R'}\n"
     ]
    }
   ],
   "source": [
    "# Pesquisando os melhores parâmetros com RandomizedSearchCV\n",
    "# Cria um dicionário com os valores que serão testados como parâmetro\n",
    "parametros = {\n",
    "   \"n_estimators\": [50, 100, 150],\n",
    "   \"learning_rate\": [0.1, 0.5, 1.0, 1.5],\n",
    "   \"algorithm\": [\"SAMME\", \"SAMME.R\"]\n",
    "}\n",
    "\n",
    "# Cria o modelo que desejamos testar os melhores parâmetros\n",
    "adaBoostClassifier = AdaBoostClassifier()\n",
    "\n",
    "# Cria o objeto do tipo RandomizedSearchCV\n",
    "randomizedSearch = RandomizedSearchCV(estimator = adaBoostClassifier, param_distributions = parametros, random_state = 11)\n",
    "\n",
    "# Treinando os parâmetros. ATENÇÃO: Deve-se usar todo conjunto de dados.\n",
    "randomizedSearch.fit(X, y)\n",
    "\n",
    "# Print do resultado\n",
    "print(\"Acurácia média: %.3f\" % (randomizedSearch.best_score_ * 100) + \"%\")\n",
    "print(\"Melhores parâmetros para o modelo:\", randomizedSearch.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando os melhores parâmetros em uma lista\n",
    "melhoresParametros = []\n",
    "for k in randomizedSearch.best_params_:\n",
    "    melhoresParametros.append(randomizedSearch.best_params_[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 86.03%\n",
      "Número de acertos: 154\n",
      "Número de erros: 25\n"
     ]
    }
   ],
   "source": [
    "# Criando o modelo\n",
    "adaBoostClassifier = AdaBoostClassifier(n_estimators=melhoresParametros[0],\n",
    "                           learning_rate=melhoresParametros[1],\n",
    "                           algorithm=melhoresParametros[2])\n",
    "\n",
    "# Treinamento do modelo\n",
    "adaBoostClassifier.fit(Xtreino, Ytreino)\n",
    "\n",
    "# Previsões com os dados de teste\n",
    "previsoes = adaBoostClassifier.predict(Xteste)\n",
    "\n",
    "# Calculando a acurácia do modelo\n",
    "numeroAcertos = accuracy_score(Yteste, previsoes, normalize=False)\n",
    "numeroErros = len(Yteste)-numeroAcertos\n",
    "acuracia = numeroAcertos/len(Yteste)\n",
    "\n",
    "print(\"Acurácia: %.2f\" % (acuracia * 100) + \"%\")\n",
    "print(\"Número de acertos:\", numeroAcertos)\n",
    "print(\"Número de erros:\", numeroErros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acurácia sem escolher os melhores valores para os parâmetros: 84.92%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algoritmo</th>\n",
       "      <th>Acurácia</th>\n",
       "      <th>Acertos</th>\n",
       "      <th>Erros</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Regressão Logística</td>\n",
       "      <td>86.03%</td>\n",
       "      <td>154</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>85.47%</td>\n",
       "      <td>153</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNN</td>\n",
       "      <td>87.71%</td>\n",
       "      <td>157</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>83.24%</td>\n",
       "      <td>149</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Árvore de Decisão</td>\n",
       "      <td>86.03%</td>\n",
       "      <td>154</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVM</td>\n",
       "      <td>87.15%</td>\n",
       "      <td>156</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>87.15%</td>\n",
       "      <td>156</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bagging Classifier</td>\n",
       "      <td>87.71%</td>\n",
       "      <td>157</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>86.03%</td>\n",
       "      <td>154</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Voting Classifier</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Gradient Tree Boosting</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Algoritmo Acurácia Acertos Erros\n",
       "0            Regressão Logística   86.03%     154    25\n",
       "1   Linear Discriminant Analysis   85.47%     153    26\n",
       "2                            KNN   87.71%     157    22\n",
       "3                    Naive Bayes   83.24%     149    30\n",
       "4              Árvore de Decisão   86.03%     154    25\n",
       "5                            SVM   87.15%     156    23\n",
       "6                  Random Forest   87.15%     156    23\n",
       "7             Bagging Classifier   87.71%     157    22\n",
       "8                       AdaBoost   86.03%     154    25\n",
       "9              Voting Classifier        -       -     -\n",
       "10        Gradient Tree Boosting        -       -     -\n",
       "11                 XGBClassifier        -       -     -"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Atualizando o dataframe com o desempenho dos algoritmos\n",
    "dfComparaAlgoritmo.iloc[8,1] = \"%.2f\" % (acuracia * 100) + \"%\"\n",
    "dfComparaAlgoritmo.iloc[8,2] = numeroAcertos\n",
    "dfComparaAlgoritmo.iloc[8,3] = numeroErros\n",
    "dfComparaAlgoritmo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifier\n",
    "<details>\n",
    "    <summary>\n",
    "        <a class=\"btnfire small stroke\"><em class=\"fas fa-chevron-circle-down\"></em>&nbsp;&nbsp;Clique para mais detalhes</a>\n",
    "    </summary>\n",
    "    <br>\n",
    "\n",
    "A ideia por trás do Voting Classifier é combinar algoritmos de aprendizado de máquina conceitualmente diferentes e usar votação majoritária ou as probabilidades previstas médias para prever as classes. O modelo final pode ser útil para equilibrar as fraquezas individuais de cada modelo. <br>\n",
    "Normalmente, não alteramos os parâmetros do algoritmo Voting Classifier e utilizamos os melhores parâmetros de cada algoritmo que será combinado.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o módulo do algoritmo\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 90.50%\n",
      "Número de acertos: 162\n",
      "Número de erros: 17\n"
     ]
    }
   ],
   "source": [
    "# Criando os modelos\n",
    "listaDeModelos = []\n",
    "\n",
    "# Cria o modelo Regressão Logística\n",
    "regressaoLogisticaVT = LogisticRegression(solver = melhoresParametrosLR[0], penalty = melhoresParametrosLR[1], C = melhoresParametrosLR[2])\n",
    "listaDeModelos.append(('Regressão Logística', regressaoLogisticaVT))\n",
    "\n",
    "# Cria o modelo Árvore de Decisão\n",
    "decisionTreeClassifierVT = DecisionTreeClassifier(splitter = melhoresParametrosAD[0], \n",
    "                                     min_samples_split = melhoresParametrosAD[1],\n",
    "                                     max_depth = melhoresParametrosAD[2],\n",
    "                                     criterion = melhoresParametrosAD[3],\n",
    "                                     random_state=14)\n",
    "listaDeModelos.append(('Árvore de Decisão', decisionTreeClassifierVT))\n",
    "\n",
    "# Cria o modelo SVM\n",
    "svcVT = SVC(kernel = melhoresParametrosSVM[0], decision_function_shape = melhoresParametrosSVM[1], C = melhoresParametrosSVM[2])\n",
    "listaDeModelos.append(('SVM', svcVT))\n",
    "\n",
    "# Criando o modelo Voting Classifier\n",
    "votingClassifier = VotingClassifier(listaDeModelos)\n",
    "\n",
    "# Treinamento do modelo\n",
    "votingClassifier.fit(Xtreino, Ytreino)\n",
    "\n",
    "# Previsões com os dados de teste\n",
    "previsoes = votingClassifier.predict(Xteste)\n",
    "\n",
    "# Calculando a acurácia do modelo\n",
    "numeroAcertos = accuracy_score(Yteste, previsoes, normalize=False)\n",
    "numeroErros = len(Yteste)-numeroAcertos\n",
    "acuracia = numeroAcertos/len(Yteste)\n",
    "\n",
    "print(\"Acurácia: %.2f\" % (acuracia * 100) + \"%\")\n",
    "print(\"Número de acertos:\", numeroAcertos)\n",
    "print(\"Número de erros:\", numeroErros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acurácia sem escolher os melhores valores para os parâmetros: 88.27%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algoritmo</th>\n",
       "      <th>Acurácia</th>\n",
       "      <th>Acertos</th>\n",
       "      <th>Erros</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Regressão Logística</td>\n",
       "      <td>86.03%</td>\n",
       "      <td>154</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>85.47%</td>\n",
       "      <td>153</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNN</td>\n",
       "      <td>87.71%</td>\n",
       "      <td>157</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>83.24%</td>\n",
       "      <td>149</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Árvore de Decisão</td>\n",
       "      <td>86.03%</td>\n",
       "      <td>154</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVM</td>\n",
       "      <td>87.15%</td>\n",
       "      <td>156</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>87.15%</td>\n",
       "      <td>156</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bagging Classifier</td>\n",
       "      <td>87.71%</td>\n",
       "      <td>157</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>86.03%</td>\n",
       "      <td>154</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Voting Classifier</td>\n",
       "      <td>90.50%</td>\n",
       "      <td>162</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Gradient Tree Boosting</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Algoritmo Acurácia Acertos Erros\n",
       "0            Regressão Logística   86.03%     154    25\n",
       "1   Linear Discriminant Analysis   85.47%     153    26\n",
       "2                            KNN   87.71%     157    22\n",
       "3                    Naive Bayes   83.24%     149    30\n",
       "4              Árvore de Decisão   86.03%     154    25\n",
       "5                            SVM   87.15%     156    23\n",
       "6                  Random Forest   87.15%     156    23\n",
       "7             Bagging Classifier   87.71%     157    22\n",
       "8                       AdaBoost   86.03%     154    25\n",
       "9              Voting Classifier   90.50%     162    17\n",
       "10        Gradient Tree Boosting        -       -     -\n",
       "11                 XGBClassifier        -       -     -"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Atualizando o dataframe com o desempenho dos algoritmos\n",
    "dfComparaAlgoritmo.iloc[9,1] = \"%.2f\" % (acuracia * 100) + \"%\"\n",
    "dfComparaAlgoritmo.iloc[9,2] = numeroAcertos\n",
    "dfComparaAlgoritmo.iloc[9,3] = numeroErros\n",
    "dfComparaAlgoritmo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Tree Boosting\n",
    "<details>\n",
    "    <summary>\n",
    "        <a class=\"btnfire small stroke\"><em class=\"fas fa-chevron-circle-down\"></em>&nbsp;&nbsp;Clique para mais detalhes</a>\n",
    "    </summary>\n",
    "    <br>\n",
    "\n",
    "O algoritmo Ensemble Gradient Tree Boosting implementa a técnica Boosting e é bastante preciso e eficaz, podendo ser utilizado em problemas de classificação e regressão. <br>\n",
    "Principais parâmetros: <br>\n",
    "- **n_estimators**: define o máximo de modelos utilizados pelo Boosting do algoritmo; <br>\n",
    "- **learning_rate**: define o peso aplicado para cada modelo de cada iteração do Boosting. Um valor alto aumenta a contribuição de cada modelo.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o módulo do algoritmo\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia média: 84.290%\n",
      "Melhores parâmetros para o modelo: {'n_estimators': 110, 'learning_rate': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# Pesquisando os melhores parâmetros com RandomizedSearchCV\n",
    "# Cria um dicionário com os valores que serão testados como parâmetro\n",
    "parametros = {\n",
    "   \"n_estimators\": [100, 110, 150],\n",
    "   \"learning_rate\": [0.1, 0.5, 1.0, 1.5],\n",
    "}\n",
    "\n",
    "# Cria o modelo que desejamos testar os melhores parâmetros\n",
    "gradientBoostingClassifier = GradientBoostingClassifier()\n",
    "\n",
    "# Cria o objeto do tipo RandomizedSearchCV\n",
    "randomizedSearch = RandomizedSearchCV(estimator = gradientBoostingClassifier, param_distributions = parametros, random_state = 11)\n",
    "\n",
    "# Treinando os parâmetros. ATENÇÃO: Deve-se usar todo conjunto de dados.\n",
    "randomizedSearch.fit(X, y)\n",
    "\n",
    "# Print do resultado\n",
    "print(\"Acurácia média: %.3f\" % (randomizedSearch.best_score_ * 100) + \"%\")\n",
    "print(\"Melhores parâmetros para o modelo:\", randomizedSearch.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando os melhores parâmetros em uma lista\n",
    "melhoresParametros = []\n",
    "for k in randomizedSearch.best_params_:\n",
    "    melhoresParametros.append(randomizedSearch.best_params_[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 88.27%\n",
      "Número de acertos: 158\n",
      "Número de erros: 21\n"
     ]
    }
   ],
   "source": [
    "# Criando o modelo\n",
    "gradientBoostingClassifier = GradientBoostingClassifier(n_estimators = melhoresParametros[0],\n",
    "                                   learning_rate=melhoresParametros[1]\n",
    "                                   )\n",
    "\n",
    "# Treinamento do modelo\n",
    "gradientBoostingClassifier.fit(Xtreino, Ytreino)\n",
    "\n",
    "# Previsões com os dados de teste\n",
    "previsoes = gradientBoostingClassifier.predict(Xteste)\n",
    "\n",
    "# Calculando a acurácia do modelo\n",
    "numeroAcertos = accuracy_score(Yteste, previsoes, normalize=False)\n",
    "numeroErros = len(Yteste)-numeroAcertos\n",
    "acuracia = numeroAcertos/len(Yteste)\n",
    "\n",
    "print(\"Acurácia: %.2f\" % (acuracia * 100) + \"%\")\n",
    "print(\"Número de acertos:\", numeroAcertos)\n",
    "print(\"Número de erros:\", numeroErros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acurácia sem escolher os melhores valores para os parâmetros: 88.27%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algoritmo</th>\n",
       "      <th>Acurácia</th>\n",
       "      <th>Acertos</th>\n",
       "      <th>Erros</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Regressão Logística</td>\n",
       "      <td>86.03%</td>\n",
       "      <td>154</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>85.47%</td>\n",
       "      <td>153</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNN</td>\n",
       "      <td>87.71%</td>\n",
       "      <td>157</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>83.24%</td>\n",
       "      <td>149</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Árvore de Decisão</td>\n",
       "      <td>86.03%</td>\n",
       "      <td>154</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVM</td>\n",
       "      <td>87.15%</td>\n",
       "      <td>156</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>87.15%</td>\n",
       "      <td>156</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bagging Classifier</td>\n",
       "      <td>87.71%</td>\n",
       "      <td>157</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>86.03%</td>\n",
       "      <td>154</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Voting Classifier</td>\n",
       "      <td>90.50%</td>\n",
       "      <td>162</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Gradient Tree Boosting</td>\n",
       "      <td>88.27%</td>\n",
       "      <td>158</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Algoritmo Acurácia Acertos Erros\n",
       "0            Regressão Logística   86.03%     154    25\n",
       "1   Linear Discriminant Analysis   85.47%     153    26\n",
       "2                            KNN   87.71%     157    22\n",
       "3                    Naive Bayes   83.24%     149    30\n",
       "4              Árvore de Decisão   86.03%     154    25\n",
       "5                            SVM   87.15%     156    23\n",
       "6                  Random Forest   87.15%     156    23\n",
       "7             Bagging Classifier   87.71%     157    22\n",
       "8                       AdaBoost   86.03%     154    25\n",
       "9              Voting Classifier   90.50%     162    17\n",
       "10        Gradient Tree Boosting   88.27%     158    21\n",
       "11                 XGBClassifier        -       -     -"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Atualizando o dataframe com o desempenho dos algoritmos\n",
    "dfComparaAlgoritmo.iloc[10,1] = \"%.2f\" % (acuracia * 100) + \"%\"\n",
    "dfComparaAlgoritmo.iloc[10,2] = numeroAcertos\n",
    "dfComparaAlgoritmo.iloc[10,3] = numeroErros\n",
    "dfComparaAlgoritmo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "<details>\n",
    "    <summary>\n",
    "        <a class=\"btnfire small stroke\"><em class=\"fas fa-chevron-circle-down\"></em>&nbsp;&nbsp;Clique para mais detalhes</a>\n",
    "    </summary>\n",
    "    <br>\n",
    "\n",
    "XGBoost não é simplesmente um algoritmo de aprendizado de máquina, mas sim, uma biblioteca projetada para ser altamente eficiente, flexível e portátil. Sua biblioteca implementa algoritmos de aprendizado de máquina sob a estrutura Gradient Boosting. A maioria dos algoritmos da biblioteca XGBoost, incluindo treinamento, previsão e avaliação, pode ser acelerada com GPUs compatíveis com CUDA. Ou seja, é uma opção fácil quando se deseja realizar o treinamento de grandes conjuntos de dados utilizando GPU. <br>\n",
    "Não é implementado pela biblioteca Scikit-Learn e não é instalado junto com a distribuição Anaconda, e por essa razão, precisa ser instalado. <br>\n",
    "Principais parâmetros: <br>\n",
    "- **tree method**: define o algoritmo de construção de árvore usado no XGBoost. Existem 4 opções de algoritmos: exact, approx, hist e gpu_hist. <br>\n",
    "- **booster**: define o Booster utilizado pelo algoritmo. As opções de Booster são gbtree, gblinear ou dart. Gbtree e dart usam modelos baseados em árvores, enquanto que gblinear utiliza funções lineares. <br>\n",
    "- **nthread**: define o número de processos paralelos utilizados durante a execução do algoritmo. <br>\n",
    "- **colsample_bynode**: define a proporção de subamostra de colunas para cada nó da árvore. A subamostragem ocorre uma vez, toda vez que um novo nó é avaliado. <br>\n",
    "- **learning_rate**: parâmetro utilizado pare prevenir o overfitting, atribuindo pezos para as variáveis. <br>\n",
    "- **max_depth**: define a profundidade máxima de uma árvore. Aumentar esse valor tornará o modelo mais complexo e com maior probabilidade de overfitting. 0 indica que não há limite de profundidade. <br>\n",
    "- **num_parallel_tree**: define o número de árvores paralelas construídas durante cada iteração. Esta opção é usada para suportar Boosted Random Forest. <br>\n",
    "- **subsample**: define a proporção de subamostras dos dados de treino. Por exemplo, ao definir a proporção de subamostras em 0.5, significa que o XGBoost criará amostras aleatoriamente em metade dos dados de treino antes de criar as árvores. Esta estratégia ajuda a reduzir o overfitting.<br> <br>\n",
    "\n",
    "Site da documentação https://xgboost.readthedocs.io/en/stable/index.html\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalando um pacote\n",
    "# !pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: xgboost\n",
      "Version: 1.7.5\n",
      "Summary: XGBoost Python Package\n",
      "Home-page: https://github.com/dmlc/xgboost\n",
      "Author: \n",
      "Author-email: \n",
      "License: Apache-2.0\n",
      "Location: c:\\users\\pinad\\anaconda3\\lib\\site-packages\n",
      "Requires: numpy, scipy\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "# Verificando a versão instalada de um pacote\n",
    "!pip show xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 89.94%\n",
      "Número de acertos: 161\n",
      "Número de erros: 18\n"
     ]
    }
   ],
   "source": [
    "# Criando o modelo\n",
    "xgb = XGBClassifier(tree_method='gpu_hist',\n",
    "                    booster='gbtree',  \n",
    "                    nthread=16,\n",
    "                    colsample_bynode= 0.8,\n",
    "                    learning_rate= 0.25,\n",
    "                    max_depth= 5,\n",
    "                    num_parallel_tree= 15,\n",
    "                    subsample= 0.5\n",
    "                   )\n",
    "\n",
    "# Criando o modelo sem acesso a GPU e com o valor padrão para os parâmetros\n",
    "# xgb = XGBClassifier()\n",
    "\n",
    "# Treinamento do modelo\n",
    "xgb.fit(Xtreino, Ytreino)\n",
    "\n",
    "# Previsões com os dados de teste\n",
    "previsoes = xgb.predict(Xteste)\n",
    "\n",
    "# Calculando a acurácia do modelo\n",
    "numeroAcertos = accuracy_score(Yteste, previsoes, normalize=False)\n",
    "numeroErros = len(Yteste)-numeroAcertos\n",
    "acuracia = numeroAcertos/len(Yteste)\n",
    "\n",
    "print(\"Acurácia: %.2f\" % (acuracia * 100) + \"%\")\n",
    "print(\"Número de acertos:\", numeroAcertos)\n",
    "print(\"Número de erros:\", numeroErros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algoritmo</th>\n",
       "      <th>Acurácia</th>\n",
       "      <th>Acertos</th>\n",
       "      <th>Erros</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Regressão Logística</td>\n",
       "      <td>86.03%</td>\n",
       "      <td>154</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>85.47%</td>\n",
       "      <td>153</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNN</td>\n",
       "      <td>87.71%</td>\n",
       "      <td>157</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>83.24%</td>\n",
       "      <td>149</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Árvore de Decisão</td>\n",
       "      <td>86.03%</td>\n",
       "      <td>154</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVM</td>\n",
       "      <td>87.15%</td>\n",
       "      <td>156</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>87.15%</td>\n",
       "      <td>156</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bagging Classifier</td>\n",
       "      <td>87.71%</td>\n",
       "      <td>157</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>86.03%</td>\n",
       "      <td>154</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Voting Classifier</td>\n",
       "      <td>90.50%</td>\n",
       "      <td>162</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Gradient Tree Boosting</td>\n",
       "      <td>88.27%</td>\n",
       "      <td>158</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>89.94%</td>\n",
       "      <td>161</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Algoritmo Acurácia Acertos Erros\n",
       "0            Regressão Logística   86.03%     154    25\n",
       "1   Linear Discriminant Analysis   85.47%     153    26\n",
       "2                            KNN   87.71%     157    22\n",
       "3                    Naive Bayes   83.24%     149    30\n",
       "4              Árvore de Decisão   86.03%     154    25\n",
       "5                            SVM   87.15%     156    23\n",
       "6                  Random Forest   87.15%     156    23\n",
       "7             Bagging Classifier   87.71%     157    22\n",
       "8                       AdaBoost   86.03%     154    25\n",
       "9              Voting Classifier   90.50%     162    17\n",
       "10        Gradient Tree Boosting   88.27%     158    21\n",
       "11                 XGBClassifier   89.94%     161    18"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Atualizando o dataframe com o desempenho dos algoritmos\n",
    "dfComparaAlgoritmo.iloc[11,1] = \"%.2f\" % (acuracia * 100) + \"%\"\n",
    "dfComparaAlgoritmo.iloc[11,2] = numeroAcertos\n",
    "dfComparaAlgoritmo.iloc[11,3] = numeroErros\n",
    "dfComparaAlgoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algoritmo</th>\n",
       "      <th>Acurácia</th>\n",
       "      <th>Acertos</th>\n",
       "      <th>Erros</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Voting Classifier</td>\n",
       "      <td>90.50%</td>\n",
       "      <td>162</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>89.94%</td>\n",
       "      <td>161</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Gradient Tree Boosting</td>\n",
       "      <td>88.27%</td>\n",
       "      <td>158</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNN</td>\n",
       "      <td>87.71%</td>\n",
       "      <td>157</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bagging Classifier</td>\n",
       "      <td>87.71%</td>\n",
       "      <td>157</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVM</td>\n",
       "      <td>87.15%</td>\n",
       "      <td>156</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>87.15%</td>\n",
       "      <td>156</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Regressão Logística</td>\n",
       "      <td>86.03%</td>\n",
       "      <td>154</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Árvore de Decisão</td>\n",
       "      <td>86.03%</td>\n",
       "      <td>154</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>86.03%</td>\n",
       "      <td>154</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>85.47%</td>\n",
       "      <td>153</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>83.24%</td>\n",
       "      <td>149</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Algoritmo Acurácia Acertos Erros\n",
       "9              Voting Classifier   90.50%     162    17\n",
       "11                 XGBClassifier   89.94%     161    18\n",
       "10        Gradient Tree Boosting   88.27%     158    21\n",
       "2                            KNN   87.71%     157    22\n",
       "7             Bagging Classifier   87.71%     157    22\n",
       "5                            SVM   87.15%     156    23\n",
       "6                  Random Forest   87.15%     156    23\n",
       "0            Regressão Logística   86.03%     154    25\n",
       "4              Árvore de Decisão   86.03%     154    25\n",
       "8                       AdaBoost   86.03%     154    25\n",
       "1   Linear Discriminant Analysis   85.47%     153    26\n",
       "3                    Naive Bayes   83.24%     149    30"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ordena o DataFrame de acordo a quantidade de acertos, em ordem descrecente\n",
    "dfComparaAlgoritmo.sort_values(by=[\"Acertos\"], ascending=False, inplace=True)\n",
    "dfComparaAlgoritmo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salvando, carregando e usando o modelo criado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>VotingClassifier(estimators=[(&#x27;Regressão Logística&#x27;,\n",
       "                              LogisticRegression(C=100, penalty=&#x27;none&#x27;)),\n",
       "                             (&#x27;Árvore de Decisão&#x27;,\n",
       "                              DecisionTreeClassifier(criterion=&#x27;log_loss&#x27;,\n",
       "                                                     max_depth=550,\n",
       "                                                     min_samples_split=6,\n",
       "                                                     random_state=14,\n",
       "                                                     splitter=&#x27;random&#x27;)),\n",
       "                             (&#x27;SVM&#x27;, SVC(kernel=&#x27;poly&#x27;))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">VotingClassifier</label><div class=\"sk-toggleable__content\"><pre>VotingClassifier(estimators=[(&#x27;Regressão Logística&#x27;,\n",
       "                              LogisticRegression(C=100, penalty=&#x27;none&#x27;)),\n",
       "                             (&#x27;Árvore de Decisão&#x27;,\n",
       "                              DecisionTreeClassifier(criterion=&#x27;log_loss&#x27;,\n",
       "                                                     max_depth=550,\n",
       "                                                     min_samples_split=6,\n",
       "                                                     random_state=14,\n",
       "                                                     splitter=&#x27;random&#x27;)),\n",
       "                             (&#x27;SVM&#x27;, SVC(kernel=&#x27;poly&#x27;))])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>Regressão Logística</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=100, penalty=&#x27;none&#x27;)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>Árvore de Decisão</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(criterion=&#x27;log_loss&#x27;, max_depth=550, min_samples_split=6,\n",
       "                       random_state=14, splitter=&#x27;random&#x27;)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>SVM</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;poly&#x27;)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "VotingClassifier(estimators=[('Regressão Logística',\n",
       "                              LogisticRegression(C=100, penalty='none')),\n",
       "                             ('Árvore de Decisão',\n",
       "                              DecisionTreeClassifier(criterion='log_loss',\n",
       "                                                     max_depth=550,\n",
       "                                                     min_samples_split=6,\n",
       "                                                     random_state=14,\n",
       "                                                     splitter='random')),\n",
       "                             ('SVM', SVC(kernel='poly'))])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecionando o modelo final\n",
    "modeloSelecionado = dfComparaAlgoritmo.iloc[0,0]\n",
    "\n",
    "if modeloSelecionado == 'Regressão Logística':\n",
    "    modeloFinal = regressaoLogistica\n",
    "elif modeloSelecionado == 'Linear Discriminant Analysis':\n",
    "    modeloFinal= linearDiscriminantAnalysis\n",
    "elif modeloSelecionado == 'KNN':\n",
    "    modeloFinal = kNeighborsClassifier\n",
    "elif modeloSelecionado == 'Naive Bayes':\n",
    "    modeloFinal = gaussianNB\n",
    "elif modeloSelecionado == 'Árvore de Decisão':\n",
    "    modeloFinal = decisionTreeClassifier\n",
    "elif modeloSelecionado == 'SVM':\n",
    "    modeloFinal = svc\n",
    "elif modeloSelecionado == 'Random Forest':\n",
    "    modeloFinal = randomForestClassifier\n",
    "elif modeloSelecionado == 'Bagging Classifier':\n",
    "    modeloFinal = baggingClassifier\n",
    "elif modeloSelecionado == 'AdaBoost':\n",
    "    modeloFinal = adaBoostClassifier\n",
    "elif modeloSelecionado == 'Voting Classifier':\n",
    "    modeloFinal = votingClassifier\n",
    "elif modeloSelecionado == 'Gradient Tree Boosting':\n",
    "    modeloFinal = gradientBoostingClassifier\n",
    "elif modeloSelecionado == 'XGBClassifier':\n",
    "    modeloFinal = xgb\n",
    "\n",
    "modeloFinal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvando e carregando o modelo\n",
    "<details>\n",
    "    <summary>\n",
    "        <a class=\"btnfire small stroke\"><em class=\"fas fa-chevron-circle-down\"></em>&nbsp;&nbsp;Clique para mais detalhes</a>\n",
    "    </summary>\n",
    "    <br>\n",
    "\n",
    "O módulo **pickle** implementa protocolos binários para serializar e desserilizar um objeto Python. **Pickling** é o processo pelo qual um objeto Python é convertido em um fluxo de bytes, e **unpickling** é a operação inversa, pela qual um fluxo de bytes de um arquivo binário ou objeto semelhante a bytes é convertido novamente em um objeto.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o módulo\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo salvo!\n"
     ]
    }
   ],
   "source": [
    "# Salvando o modelo\n",
    "arquivo = 'Dados/Titanic/modeloClassificadorFinal.sav'\n",
    "pickle.dump(modeloFinal, open(arquivo, 'wb'))\n",
    "print(\"Modelo salvo!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo carregado!\n"
     ]
    }
   ],
   "source": [
    "# Carregando o modelo\n",
    "modeloClassificador = pickle.load(open(arquivo, 'rb'))\n",
    "print(\"Modelo carregado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n",
       "       1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,\n",
       "       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fazendo previsões\n",
    "previsoes = modeloClassificador.predict(Xteste)\n",
    "previsoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
