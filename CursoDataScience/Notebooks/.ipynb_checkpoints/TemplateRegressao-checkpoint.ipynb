{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando Bibliotecas Python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Etapa 1 - Coletando os dados</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observações\n",
    "<details><summary>CLICK</summary>\n",
    "<p>\n",
    "Existem diversas considerações ao se carregar dados para o processo de Machine Learning. Por exemplo: seus dados possuem um header (cabeçalho)? Caso negativo, você vai precisar definir o título para cada coluna. Seus arquivos possuem comentários? Qual o delimitador das colunas? Alguns dados estão entre aspas, simples ou duplas?    \n",
    "\n",
    "Link com os datasets do scikit learn\n",
    "https://scikit-learn.org/stable/datasets/toy_dataset.html\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leitura de arquivo CSV\n",
    "dados = pd.read_csv('despesas.csv', sep = ',', encoding = 'utf-8')\n",
    "#ficar atento ao separador de colunas (atributo \"sep\") pode ser \";\", \",\" \"|\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leitura de arquivo EXCELL\n",
    "dados = pd.read_excel(\"dados\\df_2020.xlsx\", sheet_name=\"nome da planilha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o Dataset Boston Houses\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston() \n",
    "\n",
    "# Convertendo o dataset em um dataframe com Pandas\n",
    "dados = pd.DataFrame(boston.data, columns = boston.feature_names)\n",
    "dados['target'] = boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outra forma de converter o dataset do Scikit Learning em um dataframe com Pandas\n",
    "\n",
    "X, y = load_boston(return_X_y=True)\n",
    "\n",
    "# Convertendo o dataset em um dataframe com Pandas\n",
    "dados = pd.DataFrame(X)\n",
    "dados.columns = boston.feature_names\n",
    "\n",
    "# Adiciona uma coluna ao dataset\n",
    "dados[\"target\"] = y\n",
    "\n",
    "# Visualizando os dados\n",
    "dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Etapa 2: Explorando e Preparando os Dados</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observações\n",
    "<details><summary>CLICK</summary>\n",
    "<p>\n",
    "Também conhecido como Data Mungig, ETL, Manipulação de Dados, Data Wrangling \n",
    "O processo é cíclico. Ou seja, deve-se voltar ao início após o processamento dos dados.\n",
    "É sempre importante salvar os dados antes das transformações para comparações futuras.\n",
    "Perguntas:\n",
    "Os dados representam a população ou uma amostra?\n",
    "A fonte dos dados é primária ou secundária? <br>\n",
    "    A análise exploratória permite perceber novas relações entre as variáveis que antes não tinam sido percebida. Usar o scatter plot e a matriz de correlação para verificar a relação entre as variáveis\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizando os dados\n",
    "dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizando algumas linhas. O atributo \"n\" define a quantidade de linhas\n",
    "#dados.head()\n",
    "dados.head(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar se as variáveis estão nas colunas e se os registros estão nas linhas:\n",
    "#   1 - Converter linha em coluna: \n",
    "dados.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificar os tipos de variáveis:\n",
    "<details><summary>CLICK</summary>\n",
    "<p>\n",
    "    # Separar variáveis categóricas de variáveis numéricas <br>\n",
    "1 - qualitativas/categóricas: <br>\n",
    "    a) nominais -  Não existe uma ordem implícita (Ex.: sexo, religião, profissão) <br>\n",
    "    b) ordinais - Possuem uma ordem natural (Ex.: Escolaridade, classe social)<br>\n",
    "2 - quantitativas: <br>\n",
    "    a) discretas - pode ser obtida através de uma contagem simples (idade, ano, quantidade) <br>\n",
    "    b) contínuas - precisa de um processo para ser obtida (peso, altura, tamanho) - representadas por valores decimais <br> \n",
    "    Verificar se existe duplicação nas linhas e colunas <br>\n",
    "    Verificar se as variáveis numéricas possuem muitos valores únicos. Caso isso aconteça, talvez seja interessante converter essa variável para categórica.\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar qual o tipo de dado a variável foi classificada no momento da leitura do arquivo \n",
    "dados.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibir resumo do data set\n",
    "dados.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantitativo da variável alvo \n",
    "dados[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# countplot\n",
    "# variáveis com com até 6 seis valores diferentes podem ser variáveis categóricas.\n",
    "import seaborn as sns\n",
    "ax = sns.countplot(x = \"target\", data = dados, palette = \"Greens_d\");\n",
    "for p in ax.patches:\n",
    "    ax.annotate(str(p.get_height()), (p.get_x() + 0.15, p.get_height() + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checar se exite IDs duplicados. Comparar o resultado com a quantidade linhas do dataframe. \"id\" é o nome da coluna\n",
    "dados.id.value_counts().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica o total de valores únicos por coluna\n",
    "dados.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratando dados missing\n",
    "<details><summary>CLICK</summary>\n",
    "<p>\n",
    "    O tratamento de dados missing é obrigatório. A presença dados NA prejudica o modelo. <br>\n",
    "    Verificar se a variável com valor NA tem relação com outras variáveis antes de qualquer Modificação. <br>\n",
    "    A imputação é realizada somente em variáveis quantitativas numéricas. Para variáveis qualitativas é melhor excluir a linha.<br>\n",
    "    Quando o data set apresenta uma quantidade maior ou igual a 5%  de valores missing é melhor aplicar uma regra de imputação. Por outro lado, uma quantidade menor do que 5% é melhor apagar os dados missing.\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar se existem dados missing NA\n",
    "#dados.values.any()\n",
    "#dados.isna() \n",
    "dados.isnull().sum() # Retorna a quantida de NA por coluna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratando os dados missing\n",
    "# Retornando somente as linhas com valores NA\n",
    "dados[dados.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation 1\n",
    "# A função fillna percorre todo o dataset substituindo todos os valores NA com o valor desejado\n",
    "n = 0\n",
    "dados = dados.fillna(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation 2\n",
    "# Substitui todos os valores NA com a média da respectiva coluna\n",
    "dados = dados.fillna(dados.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apaga todas as linhas do dataset com valores NA\n",
    "dados = dados.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explorando relacionamento entre as variáveis: Matriz de Correlação\n",
    "<details><summary>CLICK</summary>\n",
    "<p>\n",
    "Observar principalmente o coeficiente de correlação da variável alvo com as outras variáveis <br>\n",
    "A correlação tem o mesmo conceito de grandezas diretamente e inversamente proporcionais. <br> \n",
    "Obs.: a correlação não gera causalidade. <br>\n",
    "Visualizando relacionamento entre as variáveis: Scatterplot (analisa a relação entre duas variáveis x (independente) e y (dependente). <br>\n",
    "    A análise exploratória permite perceber novas relações entre as variáveis que antes não tinham sido percebidas. Isso pode ajudar bastante a área de negócio.\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um Correlation Plot\n",
    "def visualize_correlation_matrix(data, hurdle = 0.0):\n",
    "    R = np.corrcoef(data, rowvar = 0)\n",
    "    R[np.where(np.abs(R) < hurdle)] = 0.0\n",
    "    heatmap = plt.pcolor(R, cmap = mpl.cm.coolwarm, alpha = 0.8)\n",
    "    heatmap.axes.set_frame_on(False)\n",
    "    heatmap.axes.set_yticks(np.arange(R.shape[0]) + 0.5, minor = False)\n",
    "    heatmap.axes.set_xticks(np.arange(R.shape[1]) + 0.5, minor = False)\n",
    "    heatmap.axes.set_xticklabels(variables, minor = False)\n",
    "    plt.xticks(rotation=90)\n",
    "    heatmap.axes.set_yticklabels(variables, minor = False)\n",
    "    plt.tick_params(axis = 'both', which = 'both', bottom = 'off', top = 'off', left = 'off', right = 'off') \n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retorna os nomes das colunas que serão utilizados no gráfico\n",
    "variables = dados.columns\n",
    "\n",
    "# Visualizando o Plot\n",
    "visualize_correlation_matrix(dados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificar se os dados estão com uma distribuição normal\n",
    "<details><summary>CLICK</summary>\n",
    "<p>\n",
    "Construir histograma (analisa apenas uma variável). <br>\n",
    "O histograma deve está simetrico  e com a curtose próxima de zero para ser utilizado no aprendizado de máquina <br>\n",
    "\n",
    "    Análise do coeficiente de assimetria: \n",
    "- skewness < 0 => simetria negativa (curva do histograma mais para esquerda)  média < mediana\n",
    "- skewness > 0 => simetria positiva (curva do histograma mais para direita) média > mediana\n",
    "- skewness aproximadmente igual a 0 => dados simétricos (curva do histograma simétrica para ambos os lados)\n",
    "   \n",
    "Análise da curtose:\n",
    "- kurtosis < 0 => curtose negativa (curva do histograma mais para baixo)\n",
    "- kurtosis > 0 => curtose positiva (curva do histograma mais para cima)\n",
    "- kurtosis aproximadamente igual a 0 => distribuição normal dos dados (curva no formato de sino)\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograma com a curva nromal\n",
    "df = norm.rvs(dados.variavelAlvo) \n",
    "sns.distplot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcular o coeficiente de assimetria em todas as variáveis\n",
    "df.skew() \n",
    "\n",
    "# apenas uma variável\n",
    "df['variavelAlvo'].skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular a curtose em todas as variáveis\n",
    "df.kurt()\n",
    "\n",
    "# apenas uma variável\n",
    "df['variavelAlvo'].kurt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seleção de variáveis (Feature Selection)\n",
    "<details><summary>CLICK</summary>\n",
    "<p>\n",
    "Nessa etapa são escolhidas as melhores variáveis que farão parte do modelo.\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizando o modelo de Regressão Linear do Stats Model para analisar o Valor-p e identificar as melhores variáveis\n",
    "# as variáveis com Valor-p < 0,05 são consideradas relevantes para o modelo\n",
    "\n",
    "# Coletando x e y\n",
    "X = dados.iloc[:,:-1]\n",
    "y = dados['target'].values\n",
    "\n",
    "# Criando e treinando modelo de Regressão Linear do Stats Model e por fim exibindo o sumário\n",
    "Xc = sm.add_constant(X)\n",
    "modeloOLS = sm.OLS(y, Xc)\n",
    "modeloOLS_v2 = modeloOLS.fit()\n",
    "modeloOLS_v2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autovalores (Eigenvalues) e Autovetores (Eigenvectors)\n",
    "Uma forma ainda mais automática de detectar associações multicolineares (e descobrir problemas numéricos em uma inversão de matriz) é usar autovetores. Explicados em termos simples, os autovetores são uma maneira muito inteligente de recombinar a variância entre as variáveis, criando novos recursos acumulando toda a variância compartilhada. Tal recombinação pode ser obtida usando a função NumPy linalg.eig, resultando em um vetor de autovalores (representando a quantidade de variância recombinada para cada nova variável) e autovetores (uma matriz nos dizendo como as novas variáveis se relacionam com as antigas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerando eigenvalues e eigenvectors\n",
    "corr = np.corrcoef(X, rowvar = 0)\n",
    "eigenvalues, eigenvectors = np.linalg.eig(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois de extrair os autovalores, imprimimos em ordem decrescente e procuramos qualquer elemento cujo valor seja próximo de zero ou pequeno em comparação com os outros. Valores próximos a zero podem representar um problema real para equações normais e outros métodos de otimização baseados na inversão matricial. Valores pequenos representam uma fonte elevada, mas não crítica, de multicolinearidade. Se você detectar qualquer um desses valores baixos, anote a posição no vetor (lembre-se que os índices em Python começam por zero). \n",
    "\n",
    "O menor valor está na posição 8. Vamos buscar a posição 8 no autovetor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvaluesPD = pd.DataFrame(eigenvalues)\n",
    "eigenvaluesPD.columns = [\"eigenvalues\"]\n",
    "# Exibe em ordem crescente\n",
    "eigenvaluesPD.sort_values(by='eigenvalues', ascending = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando a posição do índice na lista de autovalores, podemos encontrar o vetor específico nos autovetores que contém as variáveis carregadas, ou seja, o nível de associação com os valores originais. No eigenvector, observamos valores nas posições de índice 8 e 9, que estão realmente em destaque em termos de valor absoluto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvectorsPD = pd.DataFrame(eigenvectors[:,8])\n",
    "eigenvectorsPD.columns = [\"eigenvectors\"]\n",
    "# Exibe em ordem Decrescente\n",
    "eigenvectorsPD.sort_values(by=\"eigenvectors\", ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora nós imprimimos os nomes das variáveis para saber quais contribuem mais com seus valores para construir o autovetor. Associamos o vetor de variáveis com o eigenvector.\n",
    "\n",
    "Tendo encontrado os culpados da multicolinearidade, o que devemos fazer com essas variáveis? A remoção de algumas delas é geralmente a melhor solução."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (variables[8], variables[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outra alternativa para verificar as melhores variáveis é aplicar o modelo de Reressão Linear com os dados PADRONIZADOS e depois verificar os coeficientes \n",
    "\n",
    "# Criando um modelo\n",
    "modelo = linear_model.LinearRegression(normalize = False, fit_intercept = True)\n",
    "\n",
    "# Treinando o modelo com dados não padronizados (em escalas diferentes)\n",
    "modelo.fit(X,y)\n",
    "\n",
    "# Imprimindo os coeficientes e as variáveis\n",
    "# O resultado exibe os coeficientes em ordem de mais importância para o de menos importância\n",
    "for coef, var in sorted(zip(map(abs, modelo.coef_), dadosPadronizados.columns[:-1]), reverse = True):\n",
    "    print (\"%6.3f %s\" % (coef,var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "<details><summary>CLICK</summary>\n",
    "<p>\n",
    "Podemos aplicar Feature Scaling através de Padronização ou Normalização. Normalização aplica escala aos dados com intervalos entre 0 e 1. A Padronização divide a média pelo desvio padrão para obter uma unidade de variância. Vamos usar a Padronização (StandardScaler) pois nesse caso esta técnica ajusta os coeficientes e torna a superfície de erros mais \"tratável\".\n",
    "\n",
    "The Machine Learning algorithms that require the feature scaling are mostly KNN (K-Nearest Neighbours), Neural Networks, Linear Regression, and Logistic Regression.\n",
    "\n",
    "The machine learning algorithms that do not require feature scaling is mostly non-linear ML algorithms such as Decision trees, Random Forest, AdaBoost, Naïve Bayes, etc.\n",
    "    \n",
    "    O treinamento do modelo é mais rápido quando todos os dados estão com a mesma quantidade de casas decimais.\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalização - Método 1\n",
    "\n",
    "<details><summary>CLICK</summary>\n",
    "<p>\n",
    "E uma das primeiras tarefas dentro do pré-processamento, é colocar seus dados na mesma escala. Muitos algoritmos de Machine Learning vão se beneficiar disso e produzir resultados melhores. Esta etapa também é chamada de normalização e significa colocar os dados em uma escala com range entre 0 e 1. Isso é útil para a otimização, sendo usado no core dos algoritmos de Machine Learning, como gradient descent. Isso também é útil para algoritmos como regressão e redes neurais e algoritmos que usam medidas de distância, como KNN. O scikit-learn possui uma função para esta etapa, chamada MinMaxScaler().\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao entregar novos dados para realizar as previsões com o modelo treinado, os dados também devem ser normalizados. Para fazer isso basta utilizar a média encontrada no método de normalização e subtratir os novos dados por essa média."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalização dos dados pela Média\n",
    "\n",
    "# Cálculo da média do dataset de treino\n",
    "X_norm = np.mean(X, axis = 0)\n",
    "\n",
    "# Normalização dos dados de treino e de teste\n",
    "X_treino_norm = treinoData - X_norm\n",
    "X_valid_norm = validData - X_norm\n",
    "X_teste_norm = testeData - X_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformando os dados para a mesma escala (entre 0 e 1)\n",
    "\n",
    "dadosNormalizados = dados.copy()\n",
    "\n",
    "# Gerando a nova escala (normalizando os dados)\n",
    "scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "dadosNormalizados = scaler.fit_transform(dadosNormalizados)\n",
    "\n",
    "dadosNormalizados = pd.DataFrame(dadosNormalizados, columns=['CRIM', 'ZN', 'INDUS', \"CHAS\", 'NOX', 'RM', 'AGE', \"DIS\", 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'target'])\n",
    "dadosNormalizados.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalização - Método 2\n",
    "\n",
    "<details><summary>CLICK</summary>\n",
    "<p>\n",
    "No scikit-learn, normalização se refere a ajustar a escala de cada observação (linha) de modo que ela tenha comprimento igual a 1 (chamado vetor de comprimento 1 em álgebra linear). Este método de pré-processamento é útil quando temos datasets esparsos (com muitos zeros) e atributos com escala muito variada. Útil quando usamos algoritmos de redes neurais ou que usam medida de distância, como KNN. O scikit-learn possui uma função para esta etapa, chamada Normalizer().\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizando os dados (comprimento igual a 1)\n",
    "\n",
    "dadosNormalizados2 = dados.copy()\n",
    "\n",
    "# Gerando os dados normalizados\n",
    "scaler = Normalizer().fit(dadosNormalizados2)\n",
    "dadosNormalizados2 = scaler.transform(dadosNormalizados2)\n",
    "\n",
    "dadosNormalizados2 = pd.DataFrame(dadosNormalizados2, columns=['CRIM', 'ZN', 'INDUS', \"CHAS\", 'NOX', 'RM', 'AGE', \"DIS\", 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'target'])\n",
    "dadosNormalizados2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padronização\n",
    "\n",
    "<details><summary>CLICK</summary>\n",
    "<p>\n",
    "Padronização é a técnica para transformar os atributos com distribuição Gaussiana (normal) e diferentes médias e desvios padrão em uma distribuição Gaussiana com a média igual a 0 e desvio padrão igual a 1. Isso é útil para algoritmos que esperam que os dados estejam com uma distribuição Gaussiana, como regressão linear, regressão logística e linear discriminant analysis. Funciona bem quando os dados já estão na mesma escala. O scikit-learn possui uma função para esta etapa, chamada StandardScaler().\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padronizando os dados (0 para a média, 1 para o desvio padrão)\n",
    "\n",
    "dadosPadronizados = dados.copy()\n",
    "\n",
    "# Aplicando Padronização\n",
    "scaler = StandardScaler().fit(dadosPadronizados)\n",
    "dadosPadronizados = scaler.transform(dadosPadronizados)\n",
    "\n",
    "# Salvando a média e o desvio padrão\n",
    "original_means = scaler.mean_\n",
    "originanal_stds = scaler.scale_\n",
    "\n",
    "dadosPadronizados = pd.DataFrame(dadosPadronizados, columns=['CRIM', 'ZN', 'INDUS', \"CHAS\", 'NOX', 'RM', 'AGE', \"DIS\", 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'target'])\n",
    "dadosPadronizados.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desfazendo a Padronização\n",
    "# -1 indica que a variável alvo não está incluída\n",
    "unstandardized_betas = w[:-1] / originanal_stds\n",
    "unstandardized_bias  = w[-1]-np.sum((original_means / originanal_stds) * w[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construir um histograma\n",
    "# Verificar se os dados estão com uma distribuição normal\n",
    "\n",
    "# Histograma com estimativa de densidade de kernel - Distribuição univariada\n",
    "sns.distplot(dados[\"CRIM\"],  rug = True, fit = stats.gausshyper);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colocar os dados em distribuição normal utilizando transformação de log\n",
    "# Muito importante para Linear Regression e Ridge Regression. Comparar o resultado aplicando aos dados originais e aos dados normalizados\n",
    "# tratar os dados NA antes de aplicar a transformação\n",
    "dados =  np.log1p(dados)\n",
    "dadosNormalizados =  np.log1p(dadosNormalizados)\n",
    "dadosPadronizados =  np.log1p(dadosPadronizados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Etapa 3 - Dividir os dados em amostras </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando os dados de treino e dados de teste usando Cross Validation\n",
    "# Separando os valores do dataset em um vetor\n",
    "array = dadosNormalizados.values\n",
    "\n",
    "# Separando o array em componentes de input e output, considerando todas as variáveis\n",
    "X = array[:,0:12]\n",
    "Y = array[:,13]\n",
    "\n",
    "# Definindo os valores para os folds\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "\n",
    "# Separando os dados em folds\n",
    "kfold = KFold(num_folds, True, random_state = seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quanto menor o tamanho do dataset maior deve ser o tamanho dos dados de treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando os dados utilizando dados de treino e dados de teste PADRONIZADOS \n",
    "num_observ = len(dadosPadronizados)\n",
    "X = dadosPadronizados[['CRIM', 'ZN', 'INDUS', 'NOX', 'RM', 'AGE', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']].values.reshape((num_observ, 11)) # X deve sempre ser uma matriz e nunca um vetor\n",
    "Y = dadosPadronizados['target'].values # y pode ser um vetor\n",
    "\n",
    "# o random_state é útil durante os teste para reproduzir os mesmos resultados após fechar  e abrir o Jupyter Lab. Pode ser retirando na versão final do modelo.\n",
    "# Divide os dados em treino e teste\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando os dados utilizando dados de treino e dados de teste SEM NORMALIZAR OS DADOS\n",
    "# Separando o array em componentes de input e output, SELECIONANDO as variáveis\n",
    "num_observ = len(dados)\n",
    "# 11 é o número de variáveis (colunas)\n",
    "X = dados[['CRIM', 'ZN', 'INDUS', 'NOX', 'RM', 'AGE', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']].values.reshape((num_observ, 11)) # X deve sempre ser uma matriz e nunca um vetor\n",
    "Y = dados['target'].values # y pode ser um vetor\n",
    "\n",
    "# Divide os dados em treino e teste\n",
    "# o random_state é útil durante os teste para reproduzir os mesmos resultados após fechar  e abrir o Jupyter Lab. Pode ser retirando na versão final do modelo.\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando os dados utilizando dados de treino e dados de teste com os DADOS NORMALIZADOS\n",
    "# Separando o array em componentes de input e output, SELECIONANDO as variáveis\n",
    "num_observ = len(dadosNormalizados)\n",
    "# 11 é o número de variáveis (colunas)\n",
    "X = dadosNormalizados[['CRIM', 'ZN', 'INDUS', 'NOX', 'RM', 'AGE', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']].values.reshape((num_observ, 11)) # X deve sempre ser uma matriz e nunca um vetor\n",
    "Y = dadosNormalizados['target'].values # y pode ser um vetor\n",
    "\n",
    "# Divide os dados em treino e teste\n",
    "# o random_state é útil durante os teste para reproduzir os mesmos resultados após fechar  e abrir o Jupyter Lab. Pode ser retirando na versão final do modelo.\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Etapa 4 - Criando, treinando e avaliando os Modelos </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressão Linear\n",
    "\n",
    "<details><summary>CLICK</summary>\n",
    "<p>\n",
    "Assume que os dados estão em Distribuição Normal e também assume que as variáveis são relevantes para a construção do modelo e que não sejam colineares, ou seja, variáveis com alta correlação (cabe a você, Cientista de Dados, entregar ao algoritmo as variáveis realmente relevantes).\n",
    "\n",
    "Funciona melhor com os dados Padronizados do que Normalizados\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando modelo com cross validation\n",
    "\n",
    "X = dados[['CRIM', 'ZN', 'INDUS', \"CHAS\", 'NOX', 'RM', 'AGE', \"DIS\", 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']]\n",
    "Y = dados[\"target\"]\n",
    "\n",
    "# Definindo os valores para os folds\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "\n",
    "# Separando os dados em folds\n",
    "kfold = KFold(num_folds, True, random_state = seed)\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = LinearRegression()\n",
    "\n",
    "# Cross Validation\n",
    "resultado =  cross_val_score(modelo, X, Y, cv = kfold)\n",
    "\n",
    "# Print do resultado\n",
    "print(\"Score: %.3f%%\" % (resultado.mean() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando modelo com dados de treino e teste\n",
    "\n",
    "# Criando o modelo\n",
    "modelo2 = LinearRegression()\n",
    "\n",
    "# Treinando o modelo\n",
    "modelo2.fit(X_train, Y_train)\n",
    "\n",
    "# Fazendo previsões\n",
    "y_pred = modelo2.predict(X_test)\n",
    "\n",
    "# Resultado\n",
    "r2 = r2_score(Y_test, Y_pred)\n",
    "print(\"O R2 do modelo é:\", r2*100)\n",
    "\n",
    "# Cáculo do erro\n",
    "print(\"O erro do modelo é:\", mean_squared_error(Y_pred, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizando a linha de regressão do algoritmo\n",
    "\n",
    "fig = px.scatter(x=Y_test, y=y_pred, title=\"Linha de regressão do algoritmo\", labels={'x': 'Variável alvo dos dados de teste', 'y': 'Valor Previsto pelo modelo'})\n",
    "fig.add_shape(\n",
    "    type=\"line\", line=dict(dash='dash'), \n",
    "    x0=Y.min(), y0=Y.min(),\n",
    "    x1=Y.max(), y1=Y.max()\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprime os coeficientes\n",
    "print (modelo2.coef_)\n",
    "print (modelo2.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprime as previsões\n",
    "previsoes = pd.DataFrame(Y_pred)\n",
    "previsoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compara as saídas de Teste com as previsões \n",
    "targetTest = pd.DataFrame(Y_test)\n",
    "targetTest[\"previsões\"] = Y_pred\n",
    "# renomeia a primeira coluna\n",
    "targetTest.rename(columns = {0:\"Target teste\"}, inplace = True)\n",
    "targetTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazendo previsões com o modelo treinado SEM NORMALIZAR OS DADOS\n",
    "CRIM = 0.00632; ZN = 18; INDUS = 2.31; NOX = 0.538; RM =  6.575; AGE =  65.2; RAD = 1; TAX = 296; PTRATIO = 15.3; B = 396.9; LSTAT = 4.98\n",
    "\n",
    "# Lista com os novos valores das variáveis\n",
    "dadosDeEntrada = [CRIM, ZN, INDUS, NOX, RM, AGE, RAD, TAX, PTRATIO, B, LSTAT]\n",
    "Xp = np.array(dadosDeEntrada).reshape(1, -1)\n",
    "\n",
    "# Previsão\n",
    "print(\"Taxa Média de Ocupação Para a Casa:\", modelo2.predict(Xp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazendo previsões com o modelo treinado\n",
    "# Como os dados de treino foram NORMALIZADOS, os dados de entrada também devem ser normalizados\n",
    "# Apresentar ao modelo a mesma quantidade de variáveis utilizadas para testar o modelo\n",
    "CRIM = 0.000000; ZN = 0.18; INDUS = 0.067815; CHAS = 0.0; NOX = 0.314815; RM = 0.577505; DIS = 0.641607;AGE = 0.269203; RAD =0.000000; TAX = 0.208015; PTRATIO = 0.287234; B = 1.000000; LSTAT =0.089680\n",
    "\n",
    "# Lista com os novos valores das variáveis\n",
    "dadosDeEntrada = [CRIM, ZN, INDUS, CHAS, NOX, RM, AGE, DIS, RAD, TAX, PTRATIO, B, LSTAT]\n",
    "dadosDeEntrada = np.array(dadosDeEntrada).reshape(1, -1) # reshape transforma um vetor em uma linha com várias colunas\n",
    "\n",
    "# Previsão\n",
    "print(\"Taxa Média de Ocupação Para a Casa:\", modelo2.predict(dadosDeEntrada))\n",
    "#print(\"Taxa Média de Ocupação Para a Casa:\", modeloSVRDadosNorm.predict(dadosDeEntrada))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression\n",
    "\n",
    "<details><summary>CLICK</summary>\n",
    "<p>\n",
    "Extensão para a regressão linear onde a loss function é modificada para minimizar a complexidade do modelo.\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o modelo\n",
    "modeloRidge = Ridge()\n",
    "\n",
    "# Treinando o modelo\n",
    "modeloRidge.fit(X_train, Y_train)\n",
    "\n",
    "# Fazendo previsões\n",
    "Y_predRidge = modeloRidge.predict(X_test)\n",
    "\n",
    "# Resultado\n",
    "r2 = r2_score(Y_test, Y_predRidge)\n",
    "print(\"O R2 do modelo é:\", r2*100)\n",
    "\n",
    "# Cáculo do erro\n",
    "print(\"O erro do modelo é:\", mean_squared_error(Y_predRidge, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compara as saídas de Teste com as previsões \n",
    "\n",
    "targetTest = pd.DataFrame(Y_test)\n",
    "targetTest[\"previsões\"] = Y_predRidge\n",
    "# renomeia a primeira coluna\n",
    "targetTest.rename(columns = {0:\"Target teste\"}, inplace = True)\n",
    "targetTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression\n",
    "\n",
    "<details><summary>CLICK</summary>\n",
    "<p>\n",
    "Lasso (Least Absolute Shrinkage and Selection Operator) Regression é uma modificação da regressão linear e assim como a Ridge Regression, a loss function é modificada para minimizar a complexidade do modelo.\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o modelo\n",
    "modeloLasso = Lasso()\n",
    "\n",
    "# Treinando o modelo\n",
    "modeloLasso.fit(X_train, Y_train)\n",
    "\n",
    "# Fazendo previsões\n",
    "Y_predLasso = modeloLasso.predict(X_test)\n",
    "\n",
    "# Resultado\n",
    "r2 = r2_score(Y_test, Y_predLasso)\n",
    "print(\"O R2 do modelo é:\", r2*100)\n",
    "\n",
    "# Cáculo do erro\n",
    "print(\"O erro do modelo é:\", mean_squared_error(Y_predLasso, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CART\n",
    "<details><summary>CLICK</summary>\n",
    "<p>\n",
    "Funciona melhor com os dados Padronizados do que Normalizados\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o modelo\n",
    "modeloCART = DecisionTreeRegressor()\n",
    "\n",
    "# Treinando o modelo\n",
    "modeloCART.fit(X_train, Y_train)\n",
    "\n",
    "# Fazendo previsões\n",
    "Y_predCART = modeloCART.predict(X_test)\n",
    "\n",
    "# Resultado\n",
    "r2 = r2_score(Y_test, Y_predCART)\n",
    "print(\"O R2 do modelo é:\", r2*100)\n",
    "\n",
    "# Cáculo do erro\n",
    "print(\"O erro do modelo é:\", mean_squared_error(Y_predCART, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN\n",
    "<details><summary>CLICK</summary>\n",
    "<p>\n",
    "Os dados devem estar Normalizados.\n",
    "Os outliers devem ser tratados, pois influenciam bastante no algoritmo.\n",
    "É muito importante definir a quantidade de k (número de vizinhos).\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o modelo\n",
    "modeloKNN = KNeighborsRegressor()\n",
    "\n",
    "# Treinando o modelo\n",
    "modeloKNN.fit(X_train, Y_train)\n",
    "\n",
    "# Fazendo previsões\n",
    "Y_predKNN = modeloKNN.predict(X_test)\n",
    "\n",
    "# Resultado\n",
    "r2 = r2_score(Y_test, Y_predKNN)\n",
    "print(\"O R2 do modelo é:\", r2*100)\n",
    "\n",
    "# Cáculo do erro\n",
    "print(\"O erro do modelo é:\", mean_squared_error(Y_predKNN, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM\n",
    "<details><summary>CLICK</summary>\n",
    "<p>\n",
    "    É mais eficiente para problemnas de classificação. <br>\n",
    "    SVM é muito sensível aos outliers. <br>\n",
    "    SVM foram criadas para trabalhar com dados linearmente separáveis(as classes são separados por uma linha reta) e não linearmente separáveis (grade maioria dos dados). <br>\n",
    "    Não funciona bem com conjutos de dados muito grandes. <br>\n",
    "    Muito sensível aos ruídos dos dados. Por essa razão Exige a padronização dos dados. <br>\n",
    "    Parâmetros mais importante:<br>\n",
    "    - kernel<br>\n",
    "    - C<br>\n",
    "    - gamma\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o modelo\n",
    "modeloSVR = SVR()\n",
    "\n",
    "# Treinando o modelo\n",
    "modeloSVR.fit(X_train, Y_train)\n",
    "\n",
    "# Fazendo previsões\n",
    "Y_predSVR = modeloSVR.predict(X_test)\n",
    "\n",
    "# Resultado\n",
    "r2 = r2_score(Y_test, Y_predSVR)\n",
    "print(\"O R2 do modelo é:\", r2*100)\n",
    "\n",
    "# Cáculo do erro\n",
    "print(\"O erro do modelo é:\", mean_squared_error(Y_predSVR, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizando a linha de regressão do algoritmo\n",
    "\n",
    "fig = px.scatter(x=Y_test, y=Y_predSVR, title=\"Linha de regressão do algoritmo\", labels={'x': 'Variável alvo dos dados de teste', 'y': 'Valor Previsto pelo modelo'})\n",
    "fig.add_shape(\n",
    "    type=\"line\", line=dict(dash='dash'), \n",
    "    x0=Y.min(), y0=Y.min(),\n",
    "    x1=Y.max(), y1=Y.max()\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor\n",
    "<details><summary>CLICK</summary>\n",
    "<p>\n",
    "Os 4 principais parâmetros em Modelos de Random Forest são: <br>\n",
    "    \n",
    "    criterion{“gini”, “entropy”}, default=”gini” <br>\n",
    "\n",
    "n_estimators - número de árvores na floresta, quanto maior, melhor! Cada árvore é um modelo. No final é escolhido o melhor modelo <br>\n",
    "\n",
    "max depth - o padrão é 'none' e nesse caso árvores completas são criadas. Ajustando esse parâmetro pode ajudar a evitar overfitting. <br>\n",
    "\n",
    "max_features - diferentes valores devem ser testados, pois este parâmetro impacta na forma como os modelos RF distribuem os atributos pelas árvores. <br>\n",
    "\n",
    "criterion - define a forma como o algoritmo fará a divisão dos atributos e a classificação dos nós em cada árvore. <br>\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o modelo\n",
    "modeloRandForest = RandomForestRegressor()\n",
    "\n",
    "# Treinando o modelo\n",
    "modeloRandForest.fit(X_train, Y_train)\n",
    "\n",
    "# Fazendo previsões\n",
    "Y_predRandForest = modeloRandForest.predict(X_test)\n",
    "\n",
    "# Resultado\n",
    "r2 = r2_score(Y_test, Y_predRandForest)\n",
    "print(\"O R2 do modelo é:\", r2*100)\n",
    "\n",
    "# Cáculo do erro\n",
    "print(\"O erro do modelo é:\", mean_squared_error(Y_predRandForest, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging\n",
    "<details><summary>CLICK</summary>\n",
    "<p>\n",
    "    Bagging é usado para construção de múltiplos modelos (normalmente do mesmo tipo) a partir de diferentes subsets no dataset de treino. <br>\n",
    "    Um regressor Bagging é um meta-estimador ensemble que faz o fit de regressores base, cada um em subconjuntos aleatórios do conjunto de dados original e, em seguida, agrega suas previsões individuais (por votação ou por média) para formar uma previsão final.<br>\n",
    "    Tal meta-estimador pode tipicamente ser usado como uma maneira de reduzir a variância de um estimador (por exemplo, uma árvore de decisão), introduzindo a randomização em seu procedimento de construção e fazendo um ensemble (conjunto) a partir dele.<br>\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o modelo\n",
    "modeloBagging = BaggingRegressor()\n",
    "# É possível especificar qual algoritmo utilizar para criar os modelos\n",
    "#modeloBagging = BaggingRegressor(KNeighborsRegressor())\n",
    "\n",
    "# Treinando o modelo\n",
    "modeloBagging.fit(X_train, Y_train)\n",
    "\n",
    "# Fazendo previsões\n",
    "Y_predBagging = modeloBagging.predict(X_test)\n",
    "\n",
    "# Resultado\n",
    "r2 = r2_score(Y_test, Y_predBagging)\n",
    "print(\"O R2 do modelo é:\", r2*100)\n",
    "\n",
    "# Cáculo do erro\n",
    "print(\"O erro do modelo é:\", mean_squared_error(Y_predBagging, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extremely Randomized Trees (ExtraTrees)\n",
    "<details><summary>CLICK</summary>\n",
    "<p>\n",
    "    Método esemble com funcionamento muito parecido ao Randon Forest, com a diferença que as árvores são criadas de forma randômica\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o modelo\n",
    "modeloExtraTreesRegressor = ExtraTreesRegressor()\n",
    "\n",
    "# Treinando o modelo\n",
    "modeloExtraTreesRegressor.fit(X_train, Y_train)\n",
    "\n",
    "# Fazendo previsões\n",
    "y_pred = modeloExtraTreesRegressor.predict(X_test)\n",
    "\n",
    "# Resultado\n",
    "r2 = r2_score(Y_test, y_pred)\n",
    "print(\"O R2 do modelo é:\", r2*100)\n",
    "\n",
    "# Cáculo do erro\n",
    "print(\"O erro do modelo é:\", mean_squared_error(y_pred, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizando a linha de regressão do algoritmo\n",
    "\n",
    "fig = px.scatter(x=Y_test, y=y_pred, title=\"Linha de regressão do algoritmo\", labels={'x': 'Variável alvo dos dados de teste', 'y': 'Valor Previsto pelo modelo'})\n",
    "fig.add_shape(\n",
    "    type=\"line\", line=dict(dash='dash'), \n",
    "    x0=Y.min(), y0=Y.min(),\n",
    "    x1=Y.max(), y1=Y.max()\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost\n",
    "<details><summary>CLICK</summary>\n",
    "<p>\n",
    "    Um regressor AdaBoost é um meta-estimador que começa ajustando um regressor no conjunto de dados original e depois ajusta cópias adicionais do regressor no mesmo conjunto de dados, mas onde o peso das instâncias são ajustados para que os regressores subsequentes se concentrem mais em casos difíceis. <br>\n",
    "    Utiliza o peso do modelos.\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o modelo\n",
    "modeloAdaBoostReg = AdaBoostRegressor()\n",
    "\n",
    "# Treinando o modelo\n",
    "modeloAdaBoostReg.fit(X_train, Y_train)\n",
    "\n",
    "# Fazendo previsões\n",
    "Y_predAdaBoostReg = modeloAdaBoostReg.predict(X_test)\n",
    "\n",
    "# Resultado\n",
    "r2 = r2_score(Y_test, Y_predAdaBoostReg)\n",
    "print(\"O R2 do modelo é:\", r2*100)\n",
    "\n",
    "# Cáculo do erro\n",
    "print(\"O erro do modelo é:\", mean_squared_error(Y_predAdaBoostReg, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compara as saídas de Teste com as previsões \n",
    "\n",
    "targetTest = pd.DataFrame(Y_test)\n",
    "targetTest[\"previsões\"] = Y_predSVR\n",
    "# renomeia a primeira coluna\n",
    "targetTest.rename(columns = {0:\"Target teste\"}, inplace = True)\n",
    "targetTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking Regressor\n",
    "<details><summary>CLICK</summary>\n",
    "<p>\n",
    "    A generalização empilhada é um método para combinar estimadores para reduzir seus vieses. Mais precisamente, as previsões de cada estimador individual são empilhadas e usadas como entrada para um estimador final para calcular a previsão. Este estimador final é treinado por meio de validação cruzada.\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o modelo\n",
    "# Definir os algoritmos que serão empilhados\n",
    "estimators = [('rf', RandomForestRegressor(n_estimators=10)), ('svr', SVR())]\n",
    "\n",
    "modeloStacking = StackingRegressor(estimators=estimators, final_estimator=RandomForestRegressor())\n",
    "\n",
    "# Treinando o modelo\n",
    "modeloStacking.fit(X_train, Y_train)\n",
    "\n",
    "# Fazendo previsões\n",
    "previsoes = modeloStacking.predict(X_test)\n",
    "\n",
    "r2 = modeloStacking.score(X_test, Y_test)\n",
    "print(\"O R2 do modelo é:\", r2*100)\n",
    "\n",
    "# Cáculo do erro\n",
    "print(\"O erro do modelo é:\", mean_squared_error(previsoes, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Regressor\n",
    "<details><summary>CLICK</summary>\n",
    "<p>\n",
    "    \n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria o regressor\n",
    "modeloGr = GradientBoostingRegressor(n_estimators = 100, learning_rate = 0.1, max_depth = 1, random_state = 0, loss = 'ls')\n",
    "\n",
    "# Treina o regressor\n",
    "modeloGr.fit(X_train, Y_train)\n",
    "\n",
    "# Fazendo previsões\n",
    "previsoesGr = modeloGr.predict(X_test)\n",
    "\n",
    "# Resultado\n",
    "r2 = modeloGr.score(X_test, Y_test)\n",
    "print(\"O R2 do modelo é:\", r2*100)\n",
    "\n",
    "# Cáculo do erro\n",
    "print(\"O erro do modelo é:\", mean_squared_error(previsoesGr, Y_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automação dos algoritmos\n",
    "<details><summary>CLICK</summary>\n",
    "<p>\n",
    "Opção para executar todos os algoritmos e depois comparar a performance de cada um.\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma tabela\n",
    "comparativeTable = {\"Algoritmo\": [\"Linear Regression\", \"Ridge Regression\", \"Lasso  Regression\", \"CART\", \"KNN\", \"SVN\"],\n",
    "                   \"Dados Originais\": [\"-\", \"-\", \"-\", \"-\", \"-\",\"-\"],\n",
    "                   \"Dados Padronizados\": [\"-\", \"-\", \"-\", \"-\", \"-\",\"-\"],\n",
    "                   \"Dados Normalizados\": [\"-\", \"-\", \"-\", \"-\", \"-\",\"-\"]}\n",
    "\n",
    "pdComparativeTable = pd.DataFrame(comparativeTable)\n",
    "pdComparativeTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['CRIM', 'ZN', 'INDUS', \"CHAS\", 'NOX', 'RM', 'AGE', \"DIS\", 'RAD', 'PTRATIO', 'B', 'LSTAT']\n",
    "#variables = ['CRIM', 'ZN', 'INDUS', \"CHAS\", 'NOX', 'RM', 'AGE', \"DIS\", 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
    "amountVariable = len(variables)  # número de variáveis (colunas)\n",
    "num_observ = len(dados)\n",
    "counter = 1\n",
    "\n",
    "while counter <= 3:\n",
    "    algoritm = 0\n",
    "    \n",
    "    # Separando os dados em dados de treino e dados de teste\n",
    "    if counter == 1: # DADOS ORIGINAIS\n",
    "        X = dados[variables].values.reshape((num_observ, amountVariable)) # X deve sempre ser uma matriz e nunca um vetor\n",
    "        Y = dados['target'].values # y pode ser um vetor\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 5)\n",
    "        \n",
    "    elif counter == 2: # Dados PADRONIZADOS \n",
    "        X = dadosPadronizados[variables].values.reshape((num_observ, amountVariable)) # X deve sempre ser uma matriz e nunca um vetor\n",
    "        Y = dadosPadronizados['target'].values # y pode ser um vetor\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 5)\n",
    "        \n",
    "    elif counter == 3: # DADOS NORMALIZADOS\n",
    "        X = dadosNormalizados[variables].values.reshape((num_observ, amountVariable)) # X deve sempre ser uma matriz e nunca um vetor\n",
    "        Y = dadosNormalizados['target'].values # y pode ser um vetor\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 5)\n",
    "    \n",
    "    # Criando os modelos\n",
    "    while algoritm <= 5: \n",
    "        if algoritm == 0 and counter == 1: #Linear Regression e Dados Originais \n",
    "            modelo = LinearRegression()\n",
    "        elif algoritm == 0 and counter == 2: #Linear Regression e Dados Padronizados \n",
    "            modelo = LinearRegression()\n",
    "        elif algoritm == 0 and counter == 3: #Linear Regression e Dados Normalizados \n",
    "            modelo = LinearRegression()\n",
    "            \n",
    "        elif algoritm == 1 and counter == 1: #Ridge Regression e Dados Originais\n",
    "            modelo = Ridge()\n",
    "        elif algoritm == 1 and counter == 2: #Ridge Regression e Dados Padronizados\n",
    "            modelo = Ridge()\n",
    "        elif algoritm == 1 and counter == 3: #Ridge Regression e Dados Normalizados\n",
    "            modelo = Ridge()  \n",
    "            \n",
    "        elif algoritm == 2 and counter == 1: #Lasso Regression e Dados Dados Originais\n",
    "            modelo = Lasso()\n",
    "        elif algoritm == 2 and counter == 2: #Lasso Regression e Dados Padronizados\n",
    "            modelo = Lasso()\n",
    "        elif algoritm == 2 and counter == 3: #Lasso Regression e Dados Normalizados\n",
    "            modelo = Lasso()   \n",
    "            \n",
    "        elif algoritm == 3 and counter == 1: #CART e Dados Dados Originais\n",
    "            modelo = DecisionTreeRegressor()\n",
    "        elif algoritm == 3 and counter == 2: #CART e Dados Dados Padronizados\n",
    "            modelo = DecisionTreeRegressor()       \n",
    "        elif algoritm == 3 and counter == 3: #CART e Dados Dados Normalizados\n",
    "            modelo = DecisionTreeRegressor()   \n",
    "            \n",
    "        elif algoritm == 4 and counter == 1: #KNN e Dados Dados Originais\n",
    "            modelo = KNeighborsRegressor()\n",
    "        elif algoritm == 4 and counter == 2: #KNN e Dados Dados Padronizados\n",
    "            modelo = KNeighborsRegressor()\n",
    "        elif algoritm == 4 and counter == 3: #KNN e Dados Dados Normalizados\n",
    "            modelo = KNeighborsRegressor() \n",
    "            \n",
    "        elif algoritm == 5 and counter == 1: #SVR e Dados Dados Originais\n",
    "            modelo = SVR()   \n",
    "        elif algoritm == 5 and counter == 2: #SVR e Dados Dados Padronizados\n",
    "            modelo = SVR()   \n",
    "        else: #SVR e Dados Dados Normalizados\n",
    "            modelo = SVR()   \n",
    "            \n",
    "        # Treinando o modelo\n",
    "        modelo.fit(X_train, Y_train)\n",
    "\n",
    "        # Fazendo previsões\n",
    "        Y_pred = modelo.predict(X_test)\n",
    "\n",
    "        # Resultado\n",
    "        r2 = r2_score(Y_test, Y_pred)\n",
    "        \n",
    "        # FALTA SALVAR CADA MODELO CRIADO\n",
    "        # Atualizando a tabela comparativa\n",
    "        if algoritm == 0 and counter == 1: #Linear Regression e Dados Originais  \n",
    "            pdComparativeTable.iloc[0,1] = r2\n",
    "        elif algoritm == 0 and counter == 2: #Linear Regression e Dados Padronizados\n",
    "            pdComparativeTable.iloc[0,2] = r2\n",
    "        elif algoritm == 0 and counter == 3: #Linear Regression e Dados Normalizados\n",
    "            pdComparativeTable.iloc[0,3] = r2\n",
    "            \n",
    "        elif algoritm == 1 and counter == 1: #Ridge Regression e Dados Originais\n",
    "            pdComparativeTable.iloc[1,1] = r2\n",
    "        elif algoritm == 1 and counter == 2: #Ridge Regression e Dados Padronizados\n",
    "            pdComparativeTable.iloc[1,2] = r2\n",
    "        elif algoritm == 1 and counter == 3: #Ridge Regression e Dados Normalizados\n",
    "            pdComparativeTable.iloc[1,3] = r2 \n",
    "            \n",
    "        elif algoritm == 2 and counter == 1: #Lasso Regression e Dados Dados Originais\n",
    "            pdComparativeTable.iloc[2,1] = r2\n",
    "        elif algoritm == 2 and counter == 2: #Lasso Regression e Dados Padronizados\n",
    "            pdComparativeTable.iloc[2,2] = r2\n",
    "        elif algoritm == 2 and counter == 3: #Lasso Regression e Dados Normalizados\n",
    "            pdComparativeTable.iloc[2,3] = r2     \n",
    "            \n",
    "        elif algoritm == 3 and counter == 1: #CART e Dados Dados Originais\n",
    "            pdComparativeTable.iloc[3,1] = r2\n",
    "        elif algoritm == 3 and counter == 2: #CART e Dados Dados Padronizados\n",
    "            pdComparativeTable.iloc[3,2] = r2\n",
    "        elif algoritm == 3 and counter == 3: #CART e Dados Dados Normalizados\n",
    "            pdComparativeTable.iloc[3,3] = r2            \n",
    "            \n",
    "        elif algoritm == 4 and counter == 1: #KNN e Dados Dados Originais\n",
    "            pdComparativeTable.iloc[4,1] = r2\n",
    "        elif algoritm == 4 and counter == 2: #KNN e Dados Dados Padronizados\n",
    "            pdComparativeTable.iloc[4,2] = r2\n",
    "        elif algoritm == 4 and counter == 3: #KNN e Dados Dados Normalizados\n",
    "            pdComparativeTable.iloc[4,3] = r2 \n",
    "            \n",
    "        elif algoritm == 5 and counter == 1: #SVR e Dados Dados Originais\n",
    "            pdComparativeTable.iloc[5,1] = r2   \n",
    "        elif algoritm == 5 and counter == 2: #SVR e Dados Dados Padronizados\n",
    "            pdComparativeTable.iloc[5,2] = r2   \n",
    "        else: #SVR e Dados Dados Normalizados\n",
    "            pdComparativeTable.iloc[5,3] = r2\n",
    "        algoritm += 1\n",
    "    counter += 1  \n",
    "\n",
    "pdComparativeTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar o resultados dos modelos com todas as variáveis para comparar com os modelos sem as variáveis com multicolinearidade\n",
    "todasVariaveis = pdComparativeTable\n",
    "todasVariaveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando os modelos utilizando Cross Validation\n",
    "variables = ['CRIM', 'ZN', 'INDUS', \"CHAS\", 'NOX', 'RM', 'AGE', \"DIS\", 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
    "amountVariable = len(variables)  # número de variáveis (colunas)\n",
    "num_observ = len(dados)\n",
    "counter = 1\n",
    "\n",
    "# Definindo os valores para os folds\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "\n",
    "# Separando os dados em folds\n",
    "kfold = KFold(num_folds, True, random_state = seed)\n",
    "\n",
    "while counter <= 3:\n",
    "    algoritm = 0\n",
    "    \n",
    "    # Separando a variável alvo das variáveis explanatórias\n",
    "    if counter == 1: # DADOS ORIGINAIS\n",
    "        X = dados[variables]\n",
    "        Y = dados['target']\n",
    "                \n",
    "    elif counter == 2: # Dados PADRONIZADOS \n",
    "        X = dadosPadronizados[variables]\n",
    "        Y = dadosPadronizados['target']\n",
    "        \n",
    "    elif counter == 3: # DADOS NORMALIZADOS\n",
    "        X = dadosNormalizados[variables]\n",
    "        Y = dadosNormalizados['target']\n",
    "            \n",
    "    # Criando os modelos\n",
    "    while algoritm <= 5: \n",
    "        if algoritm == 0 and counter == 1: #Linear Regression e Dados Originais \n",
    "            modelo = LinearRegression()\n",
    "        elif algoritm == 0 and counter == 2: #Linear Regression e Dados Padronizados \n",
    "            modelo = LinearRegression()\n",
    "        elif algoritm == 0 and counter == 3: #Linear Regression e Dados Normalizados \n",
    "            modelo = LinearRegression()\n",
    "            \n",
    "        elif algoritm == 1 and counter == 1: #Ridge Regression e Dados Originais\n",
    "            modelo = Ridge()\n",
    "        elif algoritm == 1 and counter == 2: #Ridge Regression e Dados Padronizados\n",
    "            modelo = Ridge()\n",
    "        elif algoritm == 1 and counter == 3: #Ridge Regression e Dados Normalizados\n",
    "            modelo = Ridge()  \n",
    "            \n",
    "        elif algoritm == 2 and counter == 1: #Lasso Regression e Dados Dados Originais\n",
    "            modelo = Lasso()\n",
    "        elif algoritm == 2 and counter == 2: #Lasso Regression e Dados Padronizados\n",
    "            modelo = Lasso()\n",
    "        elif algoritm == 2 and counter == 3: #Lasso Regression e Dados Normalizados\n",
    "            modelo = Lasso()   \n",
    "            \n",
    "        elif algoritm == 3 and counter == 1: #CART e Dados Dados Originais\n",
    "            modelo = DecisionTreeRegressor()\n",
    "        elif algoritm == 3 and counter == 2: #CART e Dados Dados Padronizados\n",
    "            modelo = DecisionTreeRegressor()       \n",
    "        elif algoritm == 3 and counter == 3: #CART e Dados Dados Normalizados\n",
    "            modelo = DecisionTreeRegressor()   \n",
    "            \n",
    "        elif algoritm == 4 and counter == 1: #KNN e Dados Dados Originais\n",
    "            modelo = KNeighborsRegressor()\n",
    "        elif algoritm == 4 and counter == 2: #KNN e Dados Dados Padronizados\n",
    "            modelo = KNeighborsRegressor()\n",
    "        elif algoritm == 4 and counter == 3: #KNN e Dados Dados Normalizados\n",
    "            modelo = KNeighborsRegressor() \n",
    "            \n",
    "        elif algoritm == 5 and counter == 1: #SVR e Dados Dados Originais\n",
    "            modelo = SVR()   \n",
    "        elif algoritm == 5 and counter == 2: #SVR e Dados Dados Padronizados\n",
    "            modelo = SVR()   \n",
    "        else: #SVR e Dados Dados Normalizados\n",
    "            modelo = SVR()   \n",
    "            \n",
    "        # Cross Validation\n",
    "        resultado =  cross_val_score(modelo, X, Y, cv = kfold)\n",
    "        \n",
    "        # Atualizando a tabela comparativa\n",
    "        if algoritm == 0 and counter == 1: #Linear Regression e Dados Originais  \n",
    "            pdComparativeTable.iloc[0,1] = resultado.mean() * 100\n",
    "            modeloLinearRegresDadosOrig = modelo\n",
    "        elif algoritm == 0 and counter == 2: #Linear Regression e Dados Padronizados\n",
    "            pdComparativeTable.iloc[0,2] = resultado.mean() * 100\n",
    "            modeloLinearRegresDadosPadro = modelo\n",
    "        elif algoritm == 0 and counter == 3: #Linear Regression e Dados Normalizados\n",
    "            pdComparativeTable.iloc[0,3] = resultado.mean() * 100\n",
    "            modeloLinearRegresDadosNorm = modelo\n",
    "            \n",
    "        elif algoritm == 1 and counter == 1: #Ridge Regression e Dados Originais\n",
    "            pdComparativeTable.iloc[1,1] = resultado.mean() * 100\n",
    "            modeloRidgeRegresDadosOrig = modelo\n",
    "        elif algoritm == 1 and counter == 2: #Ridge Regression e Dados Padronizados\n",
    "            pdComparativeTable.iloc[1,2] = resultado.mean() * 100\n",
    "            modeloRidgeRegresDadosPadro = modelo\n",
    "        elif algoritm == 1 and counter == 3: #Ridge Regression e Dados Normalizados\n",
    "            pdComparativeTable.iloc[1,3] = resultado.mean() * 100 \n",
    "            modeloRidgeRegresDadosNorm = modelo\n",
    "            \n",
    "        elif algoritm == 2 and counter == 1: #Lasso Regression e Dados Dados Originais\n",
    "            pdComparativeTable.iloc[2,1] = resultado.mean() * 100\n",
    "            modeloLassoRegresDadosOrig = modelo\n",
    "        elif algoritm == 2 and counter == 2: #Lasso Regression e Dados Padronizados\n",
    "            pdComparativeTable.iloc[2,2] = resultado.mean() * 100\n",
    "            modeloLassoRegresDadosPadro = modelo\n",
    "        elif algoritm == 2 and counter == 3: #Lasso Regression e Dados Normalizados\n",
    "            pdComparativeTable.iloc[2,3] = resultado.mean() * 100     \n",
    "            modeloLassoRegresDadosNorm = modelo\n",
    "            \n",
    "        elif algoritm == 3 and counter == 1: #CART e Dados Dados Originais\n",
    "            pdComparativeTable.iloc[3,1] = resultado.mean() * 100\n",
    "            modeloCARTDadosOrig = modelo\n",
    "        elif algoritm == 3 and counter == 2: #CART e Dados Dados Padronizados\n",
    "            pdComparativeTable.iloc[3,2] = resultado.mean() * 100\n",
    "            modeloCARTDadosPadro = modelo\n",
    "        elif algoritm == 3 and counter == 3: #CART e Dados Dados Normalizados\n",
    "            pdComparativeTable.iloc[3,3] = resultado.mean() * 100 \n",
    "            modeloCARTDadosNorm = modelo\n",
    "            \n",
    "        elif algoritm == 4 and counter == 1: #KNN e Dados Dados Originais\n",
    "            pdComparativeTable.iloc[4,1] = resultado.mean() * 100\n",
    "            modeloKNNDadosOrig = modelo\n",
    "        elif algoritm == 4 and counter == 2: #KNN e Dados Dados Padronizados\n",
    "            pdComparativeTable.iloc[4,2] = resultado.mean() * 100\n",
    "            modeloKNNDadosPadro = modelo\n",
    "        elif algoritm == 4 and counter == 3: #KNN e Dados Dados Normalizados\n",
    "            pdComparativeTable.iloc[4,3] = resultado.mean() * 100\n",
    "            modeloKNNDadosNorm = modelo\n",
    "            \n",
    "        elif algoritm == 5 and counter == 1: #SVR e Dados Dados Originais\n",
    "            pdComparativeTable.iloc[5,1] = resultado.mean() * 100\n",
    "            modeloSVRDadosOrig = modelo\n",
    "        elif algoritm == 5 and counter == 2: #SVR e Dados Dados Padronizados\n",
    "            pdComparativeTable.iloc[5,2] = resultado.mean() * 100\n",
    "            modeloSVRDadosPadro = modelo\n",
    "        else: #SVR e Dados Dados Normalizados\n",
    "            pdComparativeTable.iloc[5,3] = resultado.mean() * 100\n",
    "            modeloSVRDadosNorm = modelo\n",
    "        algoritm += 1\n",
    "    counter += 1  \n",
    "\n",
    "pdComparativeTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outra = pdComparativeTable\n",
    "outra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "# Criando o gráfico\n",
    "bar1 = go.Bar(x = pdComparativeTable['Algoritmo'],\n",
    "              y = pdComparativeTable['Dados Originais'],\n",
    "              name ='Dados Originais'\n",
    "             )\n",
    "bar2 = go.Bar(x = pdComparativeTable['Algoritmo'],\n",
    "              y = pdComparativeTable['Dados Padronizados'],\n",
    "              name ='Dados Padronizados'\n",
    "             )\n",
    "bar3 = go.Bar(x = pdComparativeTable['Algoritmo'],\n",
    "              y = pdComparativeTable['Dados Normalizados'],\n",
    "              name ='Dados Normalizados'\n",
    "             )\n",
    "\n",
    "data = [bar1, bar2, bar3]\n",
    "\n",
    "# Criando Layout\n",
    "layout = go.Layout(title='Pontuação dos Algoritmos',\n",
    "                   yaxis={'title':'Pontuação'},\n",
    "                   xaxis={'title': 'Algoritmo'})\n",
    "# Criando figura que será exibida\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "# Exibindo figura/gráfico\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar o modelo escolhido\n",
    "X = dadosNormalizados[variables].values.reshape((num_observ, amountVariable)) # X deve sempre ser uma matriz e nunca um vetor\n",
    "Y = dadosNormalizados['target'].values # y pode ser um vetor\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 5)\n",
    "\n",
    "modeloSVRDadosNorm.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazer as previsões com o modelo escolhido\n",
    "Y_pred = modeloSVRDadosNorm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compara as saídas de Teste com as previsões \n",
    "targetTest = pd.DataFrame(Y_test)\n",
    "targetTest[\"previsões\"] = Y_pred\n",
    "# renomeia a primeira coluna\n",
    "targetTest.rename(columns = {0:\"Target teste\"}, inplace = True)\n",
    "targetTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Etapa 5 - Otimizando a Performance do Modelo</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# otimizando o algoritmo de Regerssão Logistica\n",
    "\n",
    "# Import dos módulos\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Definindo os valores que serão testados\n",
    "valores_grid = {'penalty': ['l1','l2'], 'C': [0.001,0.01,0.1,1,10,100,1000]}\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = LogisticRegression()\n",
    "\n",
    "# Criando o grid\n",
    "modelo_LR = GridSearchCV(estimator = modelo, param_grid = valores_grid)\n",
    "modelo_LR.fit(dadosNormalizadosPadronizados, variavelAlvo)\n",
    "\n",
    "# Print do resultado\n",
    "print(\"Acurácia: %.3f\" % (modelo_LR.best_score_ * 100))\n",
    "print(\"Melhores Parâmetros do Modelo:\\n\", modelo_LR.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Etapa 6 -  Salvando/Carregando o modelo</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Salvando o modelo\n",
    "arquivo = 'modelos/modelo_classificador_final.sav'\n",
    "pickle.dump(modelo_LR, open(arquivo, 'wb'))\n",
    "print(\"Modelo salvo!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o arquivo\n",
    "modelo_classificador_final = pickle.load(open(arquivo, 'rb'))\n",
    "#modelo_producao = modelo_classificador_final.score(X_teste, Y_teste)\n",
    "print(\"Modelo carregado!\")\n",
    "\n",
    "# Print do resultado\n",
    "#print(\"Acurácia: %.3f\" % (modelo_producao.mean() * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
